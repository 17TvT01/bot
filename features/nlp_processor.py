import re
import string
import datetime
from typing import Dict, List, Tuple, Optional, Any
from collections import Counter, defaultdict
import json

# Lazy loading for advanced NLP libraries
_spacy_nlp = None
_transformers_pipeline = None
_requests = None

def get_spacy_nlp():
    global _spacy_nlp
    if _spacy_nlp is None:
        try:
            import spacy
            _spacy_nlp = spacy.load("en_core_web_sm")
        except:
            _spacy_nlp = None
    return _spacy_nlp

def get_transformers_pipeline():
    global _transformers_pipeline
    if _transformers_pipeline is None:
        try:
            from transformers import pipeline
            _transformers_pipeline = pipeline("sentiment-analysis")
        except:
            _transformers_pipeline = None
    return _transformers_pipeline

def get_requests():
    global _requests
    if _requests is None:
        try:
            import requests
            _requests = requests
        except:
            _requests = None
    return _requests

# T·ª´ kh√≥a v√† m·∫´u cho vi·ªác nh·∫≠n di·ªán t√≠nh nƒÉng
keywords = ["hi·ªÉu", "ph√¢n t√≠ch", "ng√¥n ng·ªØ", "nlp", "x·ª≠ l√Ω", "l·ªùi n√≥i", "c·∫£m x√∫c", "√Ω ƒë·ªãnh", "tr√≠ tu·ªá nh√¢n t·∫°o", "ai"]

patterns = [
    "hi·ªÉu l·ªùi n√≥i",
    "ph√¢n t√≠ch ng√¥n ng·ªØ",
    "x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n",
    "hi·ªÉu √Ω ƒë·ªãnh",
    "ph√¢n t√≠ch c·∫£m x√∫c",
    "x√≥a ghi ch√∫",
    "x√≥a nh·∫Øc nh·ªü",
    "h·ªßy l·ªãch",
    "tr√≠ tu·ªá nh√¢n t·∫°o",
    "h·ªçc m√°y"
]

class EnhancedNLPProcessor:
    """B·ªô x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n c·∫£i ti·∫øn v·ªõi kh·∫£ nƒÉng hi·ªÉu ng·ªØ c·∫£nh"""
    
    def __init__(self):
        self.intent_patterns = self._load_enhanced_intent_patterns()
        self.entity_patterns = self._load_entity_patterns()
        self.sentiment_words = self._load_enhanced_sentiment_words()
        self.synonyms = self._load_synonyms()
        self.context_memory = []  # L∆∞u tr·ªØ ng·ªØ c·∫£nh h·ªôi tho·∫°i
        self.user_preferences = {}  # L∆∞u tr·ªØ s·ªü th√≠ch ng∆∞·ªùi d√πng
        self.user_history = []  # L∆∞u tr·ªØ l·ªãch s·ª≠ t∆∞∆°ng t√°c ng∆∞·ªùi d√πng
        self.learned_patterns = {}  # L∆∞u tr·ªØ c√°c m·∫´u ƒë√£ h·ªçc t·ª´ ng∆∞·ªùi d√πng
        self.language_preferences = ["vi", "en"]  # Ng√¥n ng·ªØ ƒë∆∞·ª£c h·ªó tr·ª£
        self.search_history = []  # L∆∞u tr·ªØ l·ªãch s·ª≠ t√¨m ki·∫øm
        
    def process_command(self, command: str) -> str:
        """X·ª≠ l√Ω l·ªánh t·ª´ ng∆∞·ªùi d√πng v·ªõi enhanced processing"""
        if not command:
            return "T√¥i c√≥ th·ªÉ gi√∫p ph√¢n t√≠ch ng√¥n ng·ªØ, nh·∫≠n di·ªán √Ω ƒë·ªãnh, v√† x·ª≠ l√Ω c√°c l·ªánh li√™n quan ƒë·∫øn nh·∫Øc nh·ªü v·ªõi kh·∫£ nƒÉng hi·ªÉu ng·ªØ c·∫£nh t·ªët h∆°n."
        
        # Enhance command v·ªõi synonyms v√† normalization
        enhanced_command = self._enhance_command(command)
        
        # Ph√¢n t√≠ch vƒÉn b·∫£n v·ªõi context
        analysis = self.analyze_text_with_context(enhanced_command)
        
        # X√°c ƒë·ªãnh √Ω ƒë·ªãnh ch√≠nh
        main_intent = self._get_main_intent(analysis)
        
        # X·ª≠ l√Ω l·ªánh li√™n quan ƒë·∫øn nh·∫Øc nh·ªü
        if self._is_reminder_related(enhanced_command, analysis):
            from features.reminder import reminder
            return reminder(enhanced_command)
        
        # Ki·ªÉm tra xem c√≥ n√™n t√¨m ki·∫øm th√¥ng tin t·ª´ b√™n ngo√†i kh√¥ng
        if self._should_search_for_information(analysis):
            # Tr√≠ch xu·∫•t truy v·∫•n t·ª´ ph√¢n t√≠ch
            query = self._extract_search_query(enhanced_command, analysis)
            # Th·ª±c hi·ªán t√¨m ki·∫øm v√† tr·∫£ v·ªÅ k·∫øt qu·∫£
            return self._search_for_information(query)
        
        # C·∫≠p nh·∫≠t context memory
        self._update_context(enhanced_command, analysis)
        
        # Hi·ªÉn th·ªã k·∫øt qu·∫£ ph√¢n t√≠ch c·∫£i ti·∫øn
        return self._format_analysis_result(analysis)
    
    def _enhance_command(self, command: str) -> str:
        """C·∫£i thi·ªán l·ªánh b·∫±ng c√°ch thay th·∫ø synonyms v√† normalize"""
        enhanced = command.lower().strip()
        
        # Thay th·∫ø synonyms
        for main_word, synonyms in self.synonyms.items():
            for synonym in synonyms:
                enhanced = enhanced.replace(synonym, main_word)
        
        # Normalize d·∫•u c√¢u v√† kho·∫£ng tr·∫Øng
        enhanced = re.sub(r'\s+', ' ', enhanced)
        enhanced = re.sub(r'[^\w\s\u00C0-\u024F\u1E00-\u1EFF]', ' ', enhanced)
        
        return enhanced.strip()
    
    def analyze_text_with_context(self, text: str) -> Dict[str, Any]:
        """Ph√¢n t√≠ch vƒÉn b·∫£n v·ªõi context awareness"""
        # Ph√¢n t√≠ch c∆° b·∫£n
        basic_analysis = {
            "intent": self.detect_enhanced_intent(text),
            "entities": self.extract_enhanced_entities(text),
            "sentiment": self.analyze_enhanced_sentiment(text),
            "keywords": self.extract_smart_keywords(text),
            "normalized_text": self.normalize_text(text),
            "reminder_action": self.analyze_reminder_action(text)
        }
        
        # Th√™m context analysis
        context_score = self._calculate_context_score(text, basic_analysis)
        complexity = self._assess_complexity(text, basic_analysis)
        confidence = self._calculate_confidence(basic_analysis)
        
        enhanced_analysis = {
            **basic_analysis,
            "context_score": context_score,
            "complexity": complexity,
            "confidence": confidence,
            "enhanced": True
        }
        
        # H·ªçc t·ª´ t∆∞∆°ng t√°c n√†y
        self._learn_from_interaction(text, enhanced_analysis)
        
        return enhanced_analysis
    
    def _search_for_information(self, query: str) -> str:
        """T√¨m ki·∫øm th√¥ng tin t·ª´ c√°c ngu·ªìn b√™n ngo√†i khi kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi"""
        try:
            # Ki·ªÉm tra xem requests c√≥ s·∫µn kh√¥ng
            requests = get_requests()
            if not requests:
                return "Xin l·ªói, t√¥i kh√¥ng th·ªÉ t√¨m ki·∫øm th√¥ng tin tr·ª±c tuy·∫øn t·∫°i th·ªùi ƒëi·ªÉm n√†y v√¨ thi·∫øu th∆∞ vi·ªán requests."
            
            # URL encode the query for use in URLs
            import urllib.parse
            encoded_query = urllib.parse.quote(query)
            
            # Th·ª≠ t√¨m ki·∫øm v·ªõi DuckDuckGo Instant Answer API
            try:
                search_url = f"https://api.duckduckgo.com/?q={encoded_query}&format=json&no_html=1&skip_disambig=1"
                response = requests.get(search_url, timeout=5)
                if response.status_code == 200:
                    data = response.json()
                    # Ki·ªÉm tra AbstractText tr∆∞·ªõc
                    if "AbstractText" in data and data["AbstractText"]:
                        # L∆∞u v√†o l·ªãch s·ª≠ t√¨m ki·∫øm
                        self.search_history.append({
                            "query": query,
                            "result": data["AbstractText"][:200] + "...",  # Gi·ªõi h·∫°n ƒë·ªô d√†i
                            "timestamp": datetime.datetime.now().isoformat()
                        })
                        
                        # Gi·ªØ ch·ªâ 20 l·ªãch s·ª≠ t√¨m ki·∫øm g·∫ßn nh·∫•t
                        if len(self.search_history) > 20:
                            self.search_history.pop(0)
                        
                        return f"T√¥i ƒë√£ t√¨m th·∫•y th√¥ng tin sau v·ªÅ '{query}':\n\n{data['AbstractText'][:500]}..."
                    
                    # N·∫øu kh√¥ng c√≥ AbstractText, ki·ªÉm tra c√°c section kh√°c
                    if "RelatedTopics" in data and data["RelatedTopics"]:
                        # L·∫•y th√¥ng tin t·ª´ ch·ªß ƒë·ªÅ li√™n quan ƒë·∫ßu ti√™n
                        first_topic = data["RelatedTopics"][0]
                        if "Text" in first_topic:
                            # L∆∞u v√†o l·ªãch s·ª≠ t√¨m ki·∫øm
                            self.search_history.append({
                                "query": query,
                                "result": first_topic["Text"][:200] + "...",  # Gi·ªõi h·∫°n ƒë·ªô d√†i
                                "timestamp": datetime.datetime.now().isoformat()
                            })
                            
                            # Gi·ªØ ch·ªâ 20 l·ªãch s·ª≠ t√¨m ki·∫øm g·∫ßn nh·∫•t
                            if len(self.search_history) > 20:
                                self.search_history.pop(0)
                            
                            return f"T√¥i ƒë√£ t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·∫øn '{query}':\n\n{first_topic['Text'][:500]}..."
            except Exception as e:
                pass  # Continue to next search method
            
            # N·∫øu DuckDuckGo kh√¥ng c√≥ k·∫øt qu·∫£, th·ª≠ Wikipedia API
            try:
                wiki_url = f"https://vi.wikipedia.org/api/rest_v1/page/summary/{encoded_query}"
                response = requests.get(wiki_url, timeout=5)
                if response.status_code == 200:
                    data = response.json()
                    if "extract" in data:
                        # L∆∞u v√†o l·ªãch s·ª≠ t√¨m ki·∫øm
                        self.search_history.append({
                            "query": query,
                            "result": data["extract"][:200] + "...",  # Gi·ªõi h·∫°n ƒë·ªô d√†i
                            "timestamp": datetime.datetime.now().isoformat()
                        })
                        
                        # Gi·ªØ ch·ªâ 20 l·ªãch s·ª≠ t√¨m ki·∫øm g·∫ßn nh·∫•t
                        if len(self.search_history) > 20:
                            self.search_history.pop(0)
                        
                        return f"T√¥i ƒë√£ t√¨m th·∫•y th√¥ng tin sau v·ªÅ '{query}' t·ª´ Wikipedia:\n\n{data['extract'][:500]}..."
            except Exception as e:
                pass  # Continue to next search method
            
            # N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin t·ª´ c√°c ngu·ªìn tr√™n
            return f"Xin l·ªói, t√¥i kh√¥ng th·ªÉ t√¨m th·∫•y th√¥ng tin v·ªÅ '{query}'. B·∫°n c√≥ th·ªÉ cung c·∫•p th√™m chi ti·∫øt kh√¥ng?"
        except Exception as e:
            return f"Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra khi t√¨m ki·∫øm th√¥ng tin: {str(e)}"
    
    def _should_search_for_information(self, analysis: Dict[str, Any]) -> bool:
        """X√°c ƒë·ªãnh xem c√≥ n√™n t√¨m ki·∫øm th√¥ng tin t·ª´ b√™n ngo√†i hay kh√¥ng"""
        # Ki·ªÉm tra xem ƒë√¢y c√≥ ph·∫£i l√† c√¢u h·ªèi kh√¥ng
        intents = analysis.get("intent", {})
        is_question = "question" in intents and intents["question"] > 0.5
        
        # Ki·ªÉm tra ƒë·ªô tin c·∫≠y c·ªßa ph√¢n t√≠ch
        confidence = analysis.get("confidence", 0)
        low_confidence = confidence < 0.4  # TƒÉng ng∆∞·ª°ng confidence m·ªôt ch√∫t
        
        # Ki·ªÉm tra xem c√≥ th·ª±c th·ªÉ n√†o kh√¥ng
        entities = analysis.get("entities", {})
        has_entities = len(entities) > 0
        
        # Ki·ªÉm tra c√°c t·ª´ kh√≥a c√¢u h·ªèi ƒë·∫∑c tr∆∞ng
        question_keywords = ["l√† g√¨", "l√† ai", "·ªü ƒë√¢u", "bao nhi√™u", "th·∫ø n√†o", "t·∫°i sao", "khi n√†o", "ai l√†"]
        text = analysis.get("normalized_text", "").lower()
        has_question_keywords = any(keyword in text for keyword in question_keywords)
        
        # Lo·∫°i tr·ª´ c√°c c√¢u h·ªèi m√† tr·ª£ l√Ω c√≥ th·ªÉ x·ª≠ l√Ω n·ªôi b·ªô
        internal_question_patterns = [
            r"m·∫•y\s+gi·ªù", 
            r"th·ªùi\s+gian",
            r"ng√†y\s+m·∫•y",
            r"h√¥m\s+nay\s+l√†\s+ng√†y",
            r"m·∫•y\s+ng√†y\s+n·ªØa"
        ]
        is_internal_question = any(re.search(pattern, text) for pattern in internal_question_patterns)
        
        # N·∫øu l√† c√¢u h·ªèi n·ªôi b·ªô, kh√¥ng t√¨m ki·∫øm th√¥ng tin
        if is_internal_question:
            return False
        
        # N·∫øu l√† c√¢u h·ªèi ho·∫∑c c√≥ t·ª´ kh√≥a c√¢u h·ªèi, n√™n t√¨m ki·∫øm th√¥ng tin
        if is_question or has_question_keywords:
            return True
        
        # N·∫øu ƒë·ªô tin c·∫≠y th·∫•p v√† c√≥ th·ª±c th·ªÉ, c≈©ng n√™n t√¨m ki·∫øm
        if low_confidence and has_entities:
            return True
            
        return False
    
    def _can_answer_with_known_info(self, analysis: Dict[str, Any]) -> bool:
        """Ki·ªÉm tra xem c√≥ th·ªÉ tr·∫£ l·ªùi v·ªõi th√¥ng tin ƒë√£ bi·∫øt kh√¥ng"""
        # ƒê√¢y l√† m·ªôt ph∆∞∆°ng th·ª©c ƒë∆°n gi·∫£n, trong th·ª±c t·∫ø c√≥ th·ªÉ ph·ª©c t·∫°p h∆°n
        # Ki·ªÉm tra xem intent c√≥ ph·∫£i l√† c√°c lo·∫°i c√≥ th·ªÉ x·ª≠ l√Ω n·ªôi b·ªô kh√¥ng
        intents = analysis.get("intent", {})
        internal_intents = ["greeting", "farewell", "thanks", "apology", "reminder", "delete_reminder", "list_reminder"]
        
        for intent, score in intents.items():
            if intent in internal_intents and score > 0.5:
                return True
        
        # Ki·ªÉm tra xem c√≥ t·ª´ kh√≥a li√™n quan ƒë·∫øn c√°c ch·ª©c nƒÉng n·ªôi b·ªô kh√¥ng
        keywords = analysis.get("keywords", [])
        internal_keywords = ["nh·∫Øc", "nh·ªõ", "l·ªãch", "gi·ªù", "th·ªùi gian", "t√≠nh", "m·ªü", "ƒë√≥ng", "ch·∫°y"]
        if any(keyword in internal_keywords for keyword in keywords):
            return True
            
        return False
    
    def _learn_from_interaction(self, text: str, analysis: Dict[str, Any]):
        """H·ªçc t·ª´ t∆∞∆°ng t√°c ng∆∞·ªùi d√πng"""
        # Th√™m v√†o l·ªãch s·ª≠ ng∆∞·ªùi d√πng
        interaction = {
            "text": text,
            "analysis": analysis,
            "timestamp": datetime.datetime.now().isoformat()
        }
        self.user_history.append(interaction)
        
        # Gi·ªØ ch·ªâ 100 l·ªãch s·ª≠ g·∫ßn nh·∫•t
        if len(self.user_history) > 100:
            self.user_history.pop(0)
        
        # H·ªçc c√°c m·∫´u m·ªõi t·ª´ √Ω ƒë·ªãnh v√† th·ª±c th·ªÉ
        intents = analysis.get("intent", {})
        entities = analysis.get("entities", {})
        
        # N·∫øu c√≥ √Ω ƒë·ªãnh r√µ r√†ng, h·ªçc t·ª´ c√°c t·ª´ kh√≥a li√™n quan
        if intents:
            main_intent = self._get_main_intent(analysis)
            keywords = analysis.get("keywords", [])
            
            # C·∫≠p nh·∫≠t m·∫´u ƒë√£ h·ªçc cho √Ω ƒë·ªãnh n√†y
            if main_intent not in self.learned_patterns:
                self.learned_patterns[main_intent] = {
                    "keywords": [],
                    "phrases": [],
                    "count": 0
                }
            
            # Th√™m t·ª´ kh√≥a m·ªõi
            for keyword in keywords:
                if keyword not in self.learned_patterns[main_intent]["keywords"]:
                    self.learned_patterns[main_intent]["keywords"].append(keyword)
            
            # Th√™m c·ª•m t·ª´ m·ªõi
            normalized_text = analysis.get("normalized_text", "")
            if normalized_text and normalized_text not in self.learned_patterns[main_intent]["phrases"]:
                self.learned_patterns[main_intent]["phrases"].append(normalized_text)
            
            # TƒÉng s·ªë l·∫ßn xu·∫•t hi·ªán
            self.learned_patterns[main_intent]["count"] += 1
    
    def _apply_learned_patterns(self, text: str, intent_scores: Dict[str, float]):
        """√Åp d·ª•ng c√°c m·∫´u ƒë√£ h·ªçc ƒë·ªÉ c·∫£i thi·ªán ph√°t hi·ªán √Ω ƒë·ªãnh"""
        # Ki·ªÉm tra c√°c m·∫´u ƒë√£ h·ªçc
        for intent, pattern_data in self.learned_patterns.items():
            # Ki·ªÉm tra t·ª´ kh√≥a ƒë√£ h·ªçc
            learned_keywords = pattern_data.get("keywords", [])
            keyword_matches = sum(1 for keyword in learned_keywords if keyword in text)
            
            # Ki·ªÉm tra c·ª•m t·ª´ ƒë√£ h·ªçc
            learned_phrases = pattern_data.get("phrases", [])
            phrase_matches = sum(1 for phrase in learned_phrases if phrase in text)
            
            # T√≠nh ƒëi·ªÉm d·ª±a tr√™n c√°c m·∫´u ƒë√£ h·ªçc
            if keyword_matches > 0 or phrase_matches > 0:
                # T√≠nh ƒëi·ªÉm h·ªçc t·∫≠p d·ª±a tr√™n s·ªë l·∫ßn xu·∫•t hi·ªán
                learning_weight = min(pattern_data.get("count", 0) / 10.0, 1.0)
                learned_score = (keyword_matches * 0.1 + phrase_matches * 0.2) * learning_weight
                
                # C·∫≠p nh·∫≠t ƒëi·ªÉm √Ω ƒë·ªãnh
                if intent in intent_scores:
                    intent_scores[intent] = min(intent_scores[intent] + learned_score, 1.0)
                else:
                    intent_scores[intent] = min(learned_score, 1.0)
        
        return intent_scores
    
    def detect_enhanced_intent(self, text: str) -> Dict[str, float]:
        """Ph√°t hi·ªán √Ω ƒë·ªãnh ƒë∆∞·ª£c c·∫£i ti·∫øn v·ªõi scoring system v√† h·ªçc m√°y"""
        intent_scores = {}
        
        # Ki·ªÉm tra c√°c pattern v·ªõi tr·ªçng s·ªë kh√°c nhau
        for intent, patterns in self.intent_patterns.items():
            max_score = 0.0
            pattern_matches = 0
            
            for pattern in patterns:
                if re.search(pattern, text, re.IGNORECASE):
                    pattern_matches += 1
                    # Scoring system c·∫£i ti·∫øn
                    base_score = 0.4
                    match_bonus = pattern_matches * 0.15
                    score = min(base_score + match_bonus, 1.0)
                    max_score = max(max_score, score)
            
            if max_score > 0:
                intent_scores[intent] = max_score
        
        # X·ª≠ l√Ω ƒë·∫∑c bi·ªát cho c√°c intent ph·ª©c t·∫°p
        if any(word in text for word in ["x√≥a", "h·ªßy", "delete", "remove"]) and any(word in text for word in ["nh·∫Øc", "ghi ch√∫", "l·ªãch", "reminder"]):
            intent_scores["delete_reminder"] = 0.95
        
        if any(word in text for word in ["xem", "hi·ªÉn th·ªã", "list", "show"]) and any(word in text for word in ["nh·∫Øc", "ghi ch√∫", "l·ªãch", "reminder"]):
            intent_scores["list_reminder"] = 0.95
            
        # √Åp d·ª•ng c√°c m·∫´u ƒë√£ h·ªçc
        intent_scores = self._apply_learned_patterns(text, intent_scores)
            
        # Fallback mechanism
        if not intent_scores:
            # Ph√¢n t√≠ch d·ª±a tr√™n c·∫•u tr√∫c c√¢u
            if "?" in text or text.endswith("kh√¥ng"):
                intent_scores["question"] = 0.6
            elif any(word in text for word in ["l√†m", "t·∫°o", "gi√∫p", "m·ªü"]):
                intent_scores["command"] = 0.6
            else:
                intent_scores["unknown"] = 0.8
            
        return intent_scores
        
    def _load_enhanced_intent_patterns(self) -> Dict[str, List[str]]:
        """T·∫£i c√°c m·∫´u nh·∫≠n di·ªán √Ω ƒë·ªãnh ƒë∆∞·ª£c c·∫£i ti·∫øn"""
        return {
            "question": [
                r"^(?:ai|b·∫°n|tr·ª£ l√Ω)\s+(?:c√≥ th·ªÉ|bi·∫øt|cho\s+(?:t√¥i|m√¨nh|tao|t·ªõ)\s+(?:bi·∫øt|xem))\s+",
                r"^(?:t√¥i|m√¨nh|tao|t·ªõ)\s+(?:mu·ªën|c·∫ßn|th√≠ch)\s+(?:bi·∫øt|xem|t√¨m)\s+",
                r"(?:l√†\s+(?:g√¨|sao|th·∫ø n√†o))",
                r"(?:nh∆∞\s+th·∫ø\s+n√†o)",
                r"(?:·ªü\s+ƒë√¢u)",
                r"(?:khi\s+n√†o)",
                r"(?:ai|ng∆∞·ªùi n√†o)\s+(?:l√†|ƒë√£|s·∫Ω)",
                r"(?:t·∫°i\s+sao|v√¨\s+sao|t·∫°i\s+v√¨|v√¨|b·ªüi\s+v√¨)",
                r"(?:th·∫ø\s+n√†o|ra\s+sao|nh∆∞\s+th·∫ø\s+n√†o)",
                r"(?:c√≥|kh√¥ng)\s+(?:ph·∫£i|ƒë√∫ng|th·∫≠t)\s+(?:l√†|kh√¥ng)",
                r"(?:m·∫•y|bao\s+nhi√™u)\s+(?:gi·ªù|ph√∫t|gi√¢y|ng√†y|th√°ng|nƒÉm)",
                r"\?\s*$",  # K·∫øt th√∫c b·∫±ng d·∫•u h·ªèi
                r"(?:gi·∫£i th√≠ch|gi·∫£i th√≠ch cho t√¥i|gi·∫£i th√≠ch gi√∫p t√¥i)",
                r"(?:√Ω nghƒ©a|nghƒ©a c·ªßa)",
                r"(?:c√≥ th·ªÉ cho t√¥i bi·∫øt|c√≥ th·ªÉ gi·∫£i th√≠ch)",
                r"(?:l√†m sao|l√†m th·∫ø n√†o)\s+(?:ƒë·ªÉ|c√≥ th·ªÉ)"
            ],
            "command": [
                r"^(?:h√£y|vui\s+l√≤ng)\s+",
                r"^(?:t√¥i|m√¨nh|tao|t·ªõ)\s+(?:mu·ªën|c·∫ßn|th√≠ch)\s+(?:b·∫°n|tr·ª£ l√Ω)\s+",
                r"^(?:gi√∫p|gi√∫p\s+(?:t√¥i|m√¨nh|tao|t·ªõ))\s+",
                r"^(?:l√†m|t·∫°o|vi·∫øt|t√≠nh|t√¨m|m·ªü|ƒë√≥ng|l∆∞u|x√≥a)\s+",
                r"^(?:cho\s+(?:t√¥i|m√¨nh|tao|t·ªõ)\s+(?:xem|bi·∫øt))\s+",
                r"^(?:kh·ªüi ƒë·ªông|ch·∫°y|start|run|execute)",
                r"(?:th·ª±c hi·ªán|th·ª±c thi|ch·∫°y)\s+(?:·ª©ng d·ª•ng|ch∆∞∆°ng tr√¨nh)",
                r"(?:c√†i ƒë·∫∑t|c·∫•u h√¨nh|thi·∫øt l·∫≠p)\s+",
                r"(?:c·∫≠p nh·∫≠t|s·ª≠a ƒë·ªïi|thay ƒë·ªïi)\s+"
            ],
            "reminder": [
                r"(?:nh·∫Øc|nh·∫Øc nh·ªü|remind|reminder)\s+(?:t√¥i|m√¨nh|tao|t·ªõ)",
                r"(?:ƒë·∫∑t|t·∫°o|th√™m)\s+(?:nh·∫Øc nh·ªü|l·ªãch|reminder)",
                r"(?:l√™n l·ªãch|schedule)\s+",
                r"(?:ghi ch√∫|note)\s+(?:v·ªÅ|cho|r·∫±ng)",
                r"(?:nh·ªõ|remember)\s+(?:r·∫±ng|l√†|that)",
                r"(?:b√°o th·ª©c|alarm)\s+(?:l√∫c|v√†o l√∫c)",
                r"(?:l·ªãch tr√¨nh|schedule)\s+(?:cho|c·ªßa)",
                r"(?:s·ª± ki·ªán|event)\s+(?:s·∫Øp t·ªõi|ti·∫øp theo)"
            ],
            "delete_reminder": [
                r"(?:x√≥a|h·ªßy|lo·∫°i b·ªè|delete|remove)\s+(?:nh·∫Øc nh·ªü|l·ªãch|reminder|ghi ch√∫)",
                r"(?:b·ªè|g·ª°)\s+(?:nh·∫Øc nh·ªü|l·ªãch|reminder)",
                r"(?:hu·ª∑|cancel)\s+(?:l·ªãch|appointment|meeting)",
                r"(?:kh√¥ng\s+c·∫ßn|b·ªè qua)\s+(?:nh·∫Øc nh·ªü|reminder)",
                r"(?:x√≥a\s+b·ªè|x√≥a\s+h·∫øt)\s+(?:t·∫•t c·∫£|to√†n b·ªô)\s+(?:nh·∫Øc nh·ªü|l·ªãch)",
                r"(?:d·ªçn d·∫πp|clear)\s+(?:l·ªãch|nh·∫Øc nh·ªü)"
            ],
            "list_reminder": [
                r"(?:xem|hi·ªÉn th·ªã|show|list)\s+(?:nh·∫Øc nh·ªü|l·ªãch|reminder)",
                r"(?:c√≥\s+(?:g√¨|nh·ªØng\s+g√¨|nh·∫Øc nh·ªü\s+n√†o))",
                r"(?:danh s√°ch|list)\s+(?:nh·∫Øc nh·ªü|l·ªãch|reminder)",
                r"(?:ki·ªÉm tra|check)\s+(?:l·ªãch|calendar|schedule)",
                r"(?:xem\s+l·∫°i|xem\s+l·∫°i\s+c√°c)\s+(?:s·ª± ki·ªán|l·ªãch tr√¨nh)",
                r"(?:nh·∫Øc nh·ªü\s+g·∫ßn\s+ƒë√¢y|nh·∫Øc nh·ªü\s+s·∫Øp\s+t·ªõi)"
            ],
            "greeting": [
                r"^(?:xin\s+ch√†o|ch√†o|hello|hi|hey|good\s+morning|good\s+afternoon|good\s+evening)\s*$",
                r"^(?:ch√†o\s+bu·ªïi\s+(?:s√°ng|tr∆∞a|chi·ªÅu|t·ªëi))\s*$",
                r"^(?:ch√†o\s+m·ª´ng|welcome)\s+(?:b·∫°n|quay l·∫°i)",
                r"^(?:bu·ªïi\s+s√°ng|bu·ªïi\s+tr∆∞a|bu·ªïi\s+chi·ªÅu|bu·ªïi\s+t·ªëi)\s+(?:t·ªët l√†nh|t·ªët|ƒë·∫πp tr·ªùi)"
            ],
            "farewell": [
                r"^(?:t·∫°m\s+bi·ªát|goodbye|bye|see\s+you|h·∫πn\s+g·∫∑p\s+l·∫°i)\s*$",
                r"^(?:ch√∫c\s+(?:ng·ªß\s+ngon|ng·ªß\s+ngon|ngon\s+gi·∫•c))\s*$",
                r"^(?:t·∫°m\s+t·∫°m|bye\s+bye|t·∫°m\s+bi·ªát\s+t·∫°m\s+th·ªùi)\s*$",
                r"^(?:ƒëi\s+ng·ªß|ng·ªß\s+ƒëi|ƒëi\s+ngh·ªâ)\s+(?:r·ªìi|ƒë√¢y)"
            ],
            "thanks": [
                r"^(?:c·∫£m\s+∆°n|thank|thanks|thank\s+you|c√°m\s+∆°n)\s*",
                r"^(?:c·∫£m\s+∆°n\s+(?:b·∫°n|tr·ª£ l√Ω|nhi·ªÅu|r·∫•t|v√¥ c√πng))\s*",
                r"^(?:r·∫•t\s+bi·∫øt\s+∆°n|bi·∫øt\s+∆°n|c·∫£m\s+k√≠ch)\s*",
                r"^(?:tuy·ªát\s+v·ªùi|t·ªët\s+l·∫Øm|hay\s+l·∫Øm)\s+(?:c·∫£m\s+∆°n)"
            ],
            "apology": [
                r"^(?:xin\s+l·ªói|sorry|i'm\s+sorry|i\s+apologize)\s*",
                r"^(?:t√¥i|m√¨nh|tao|t·ªõ)\s+(?:xin\s+l·ªói)\s*",
                r"^(?:r·∫•t\s+xin\s+l·ªói|th√†nh\s+th·∫≠t\s+xin\s+l·ªói|xin\s+l·ªói\s+nhi·ªÅu)\s*",
                r"^(?:xin\s+l·ªói\s+v√¨|l·ªói\s+c·ªßa\s+t√¥i)"
            ],
            "ai_enhancement": [
                r"(?:tr√≠\s+tu·ªá\s+nh√¢n\s+t·∫°o|ai|tr√≠\s+tu·ªá\s+m√°y\s+t√≠nh)",
                r"(?:h·ªçc\s+m√°y|h·ªçc\s+s√¢u|machine\s+learning|deep\s+learning)",
                r"(?:d·ª±\s+ƒëo√°n|g·ª£i\s+√Ω|d·ª±\s+b√°o)",
                r"(?:h·ªçc\s+t·ª´|t·ª±\s+h·ªçc|c·∫£i\s+thi·ªán)",
                r"(?:th√¥ng\s+minh|h∆°n|n√¢ng\s+c·∫•p)"
            ]
        }
    
    def _load_entity_patterns(self) -> Dict[str, List[str]]:
        """T·∫£i c√°c m·∫´u nh·∫≠n di·ªán th·ª±c th·ªÉ"""
        return {
            "date": [
                r"(?:ng√†y|h√¥m)\s+(?:nay|mai|kia|m·ªët)",
                r"(?:tu·∫ßn|th√°ng|nƒÉm)\s+(?:n√†y|sau|tr∆∞·ªõc|t·ªõi|qua)",
                r"(?:\d{1,2})[-/](?:\d{1,2})(?:[-/](?:\d{2,4}))?",
                r"(?:th·ª©\s+(?:hai|ba|t∆∞|nƒÉm|s√°u|b·∫£y)|ch·ªß\s+nh·∫≠t)"
            ],
            "time": [
                r"(?:\d{1,2})\s*(?:gi·ªù|h|:|g)\s*(?:\d{1,2})?\s*(?:ph√∫t|p|')?",
                r"(?:s√°ng|tr∆∞a|chi·ªÅu|t·ªëi|ƒë√™m|khuya)",
                r"(?:b√¢y\s+gi·ªù|hi·ªán\s+t·∫°i|l√∫c\s+n√†y)"
            ],
            "person": [
                r"(?:anh|ch·ªã|c√¥|ch√∫|b√°c|√¥ng|b√†)\s+[A-Z√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏ƒê][a-z√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]*",
                r"(?:Mr\.|Mrs\.|Ms\.|Dr\.)\s+[A-Z][a-z]*"
            ],
            "location": [
                r"(?:t·∫°i|·ªü|trong|ngo√†i|g·∫ßn|xa)\s+(?:[A-Z√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏ƒê][a-z√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]*)",
                r"(?:th√†nh\s+ph·ªë|t·ªânh|qu·∫≠n|huy·ªán|ph∆∞·ªùng|x√£|ƒë∆∞·ªùng|ph·ªë)\s+(?:[A-Z√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏ƒê][a-z√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]*)",
                r"(?:vƒÉn phong|office|ph√≤ng\s+h·ªçp|meeting\s+room|h·ªôi tr∆∞·ªùng|auditorium)"
            ],
            "number": [
                r"\d+(?:[,.\s]\d+)*(?:\s*(?:tri·ªáu|ngh√¨n|t·ª∑|trƒÉm|ch·ª•c))?",
                r"(?:m·ªôt|hai|ba|b·ªën|nƒÉm|s√°u|b·∫£y|t√°m|ch√≠n|m∆∞·ªùi)(?:\s+(?:m∆∞·ªùi|m∆∞∆°i|trƒÉm|ngh√¨n|tri·ªáu|t·ª∑))*",
                r"(?:v√†i|nhi·ªÅu|√≠t|ƒë√¥i|c·∫∑p|t√°|ch·ª•c)"
            ],
            "priority": [
                r"(?:quan\s+tr·ªçng|∆∞u\s+ti√™n|kh·∫©n\s+c·∫•p|urgent|important|high|medium|low)",
                r"(?:r·∫•t\s+quan\s+tr·ªçng|c·ª±c\s+k·ª≥\s+quan\s+tr·ªçng|critical)"
            ],
            "duration": [
                r"(?:\d+)\s+(?:ph√∫t|gi·ªù|ng√†y|tu·∫ßn|th√°ng|nƒÉm)",
                r"(?:n·ª≠a|m·ªôt\s+n·ª≠a)\s+(?:ph√∫t|gi·ªù|ng√†y|tu·∫ßn|th√°ng|nƒÉm)",
                r"(?:t·ª´|trong\s+v√≤ng|kho·∫£ng|x·∫•p x·ªâ)\s+(?:\d+)\s+(?:ph√∫t|gi·ªù|ng√†y)"
            ]
        }
    
    def _load_enhanced_sentiment_words(self) -> Dict[str, float]:
        """T·∫£i t·ª´ ƒëi·ªÉn c·∫£m x√∫c ƒë∆∞·ª£c m·ªü r·ªông"""
        positive = {
            # C∆° b·∫£n
            "t·ªët": 0.8, "hay": 0.7, "tuy·ªát": 0.9, "tuy·ªát v·ªùi": 1.0, "xu·∫•t s·∫Øc": 1.0,
            "th√≠ch": 0.7, "y√™u": 0.9, "th√∫ v·ªã": 0.6, "h√†i l√≤ng": 0.8, "vui": 0.7,
            "h·∫°nh ph√∫c": 0.9, "h√†i h∆∞·ªõc": 0.6, "d·ªÖ ch·ªãu": 0.5, "tho·∫£i m√°i": 0.6,
            "ƒë·∫πp": 0.7, "xinh": 0.6, "ƒë√°ng y√™u": 0.8, "th√¥ng minh": 0.7,
            "nhanh": 0.6, "hi·ªáu qu·∫£": 0.7, "ch√≠nh x√°c": 0.8, "ƒë√∫ng": 0.7,
            # M·ªü r·ªông
            "ho√†n h·∫£o": 1.0, "tuy·ªát v·ªùi": 1.0, "·∫•n t∆∞·ª£ng": 0.8, "t√†i gi·ªèi": 0.9,
            "cool": 0.7, "awesome": 0.9, "amazing": 1.0, "excellent": 1.0,
            "fantastic": 1.0, "wonderful": 0.9, "great": 0.8, "good": 0.7,
            "nice": 0.6, "fine": 0.5, "ok": 0.3, "okay": 0.3,
            "h·ªØu √≠ch": 0.8, "ti·ªán l·ª£i": 0.7, "thu·∫≠n ti·ªán": 0.6, "d·ªÖ d√†ng": 0.6
        }
        
        negative = {
            # C∆° b·∫£n  
            "t·ªá": -0.8, "k√©m": -0.7, "d·ªü": -0.6, "ch√°n": -0.5, "bu·ªìn": -0.6,
            "kh√≥ ch·ªãu": -0.7, "b·ª±c": -0.8, "gi·∫≠n": -0.9, "gh√©t": -0.9, "s·ª£": -0.7,
            "lo": -0.5, "lo l·∫Øng": -0.6, "th·∫•t v·ªçng": -0.8, "t·ªìi": -0.7,
            "ch·∫≠m": -0.6, "sai": -0.7, "l·ªói": -0.8, "h·ªèng": -0.9,
            "x·∫•u": -0.7, "kinh kh·ªßng": -0.9, "t·ªìi t·ªá": -1.0, "kh·ªßng khi·∫øp": -1.0,
            # M·ªü r·ªông
            "th·∫•t b·∫°i": -0.9, "thua": -0.7, "kh√¥ng t·ªët": -0.6, "kh√¥ng hay": -0.5,
            "boring": -0.5, "bad": -0.7, "terrible": -0.9, "awful": -0.9,
            "horrible": -1.0, "disgusting": -1.0, "hate": -0.9, "angry": -0.8,
            "sad": -0.6, "disappointed": -0.8, "frustrated": -0.7, "annoyed": -0.6,
            "ph·ª©c t·∫°p": -0.4, "kh√≥ khƒÉn": -0.6, "r·∫Øc r·ªëi": -0.5, "phi·ªÅn ph·ª©c": -0.5
        }
        
        # K·∫øt h·ª£p t·ª´ ƒëi·ªÉn
        sentiment_dict = {}
        sentiment_dict.update(positive)
        sentiment_dict.update(negative)
        return sentiment_dict
    
    def _load_synonyms(self) -> Dict[str, List[str]]:
        """T·∫£i t·ª´ ƒëi·ªÉn t·ª´ ƒë·ªìng nghƒ©a"""
        return {
            "x√≥a": ["h·ªßy", "lo·∫°i b·ªè", "g·ª° b·ªè", "remove", "delete", "xo√°"],
            "t·∫°o": ["l√†m", "t·∫°o ra", "thi·∫øt l·∫≠p", "create", "make", "build"],
            "xem": ["hi·ªÉn th·ªã", "li·ªát k√™", "ki·ªÉm tra", "show", "view", "list", "display"],
            "nh·∫Øc nh·ªü": ["reminder", "ghi ch√∫", "note", "l·ªãch", "calendar", "s·ª± ki·ªán", "event"],
            "m·ªü": ["kh·ªüi ƒë·ªông", "ch·∫°y", "start", "open", "launch", "run"],
            "gi·ªù": ["th·ªùi gian", "time", "hour"],
            "ng√†y mai": ["tomorrow", "mai"],
            "h√¥m nay": ["today", "nay"],
            "tu·∫ßn n√†y": ["this week"],
            "th√°ng n√†y": ["this month"],
            "quan tr·ªçng": ["important", "∆∞u ti√™n", "priority", "urgent", "kh·∫©n c·∫•p"],
            "cu·ªôc h·ªçp": ["meeting", "h·ªçp", "h·ªôi ngh·ªã", "conference"],
            "c√¥ng vi·ªác": ["work", "task", "job", "nhi·ªám v·ª•", "vi·ªác"],
            "t√≠nh to√°n": ["calculate", "t√≠nh", "compute", "math"]
        }

    def analyze_text(self, text: str) -> Dict[str, Any]:
        """Ph√¢n t√≠ch vƒÉn b·∫£n ƒë·∫ßu v√†o v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ ph√¢n t√≠ch"""
        text = text.lower().strip()
        
        # K·∫øt qu·∫£ ph√¢n t√≠ch
        analysis = {
            "intent": self.detect_intent(text),
            "entities": self.extract_entities(text),
            "sentiment": self.analyze_sentiment(text),
            "keywords": self.extract_keywords(text),
            "normalized_text": self.normalize_text(text),
            "reminder_action": self.analyze_reminder_action(text)
        }
        
        return analysis
        
    def detect_enhanced_intent(self, text: str) -> Dict[str, float]:
        """Ph√°t hi·ªán √Ω ƒë·ªãnh ƒë∆∞·ª£c c·∫£i ti·∫øn v·ªõi scoring system"""
        intent_scores = {}
        
        # Ki·ªÉm tra c√°c pattern v·ªõi tr·ªçng s·ªë kh√°c nhau
        for intent, patterns in self.intent_patterns.items():
            max_score = 0.0
            pattern_matches = 0
            
            for pattern in patterns:
                if re.search(pattern, text, re.IGNORECASE):
                    pattern_matches += 1
                    # Scoring system c·∫£i ti·∫øn
                    base_score = 0.4
                    match_bonus = pattern_matches * 0.15
                    score = min(base_score + match_bonus, 1.0)
                    max_score = max(max_score, score)
            
            if max_score > 0:
                intent_scores[intent] = max_score
        
        # X·ª≠ l√Ω ƒë·∫∑c bi·ªát cho c√°c intent ph·ª©c t·∫°p
        if any(word in text for word in ["x√≥a", "h·ªßy", "delete", "remove"]) and any(word in text for word in ["nh·∫Øc", "ghi ch√∫", "l·ªãch", "reminder"]):
            intent_scores["delete_reminder"] = 0.95
        
        if any(word in text for word in ["xem", "hi·ªÉn th·ªã", "list", "show"]) and any(word in text for word in ["nh·∫Øc", "ghi ch√∫", "l·ªãch", "reminder"]):
            intent_scores["list_reminder"] = 0.95
            
        # Fallback mechanism
        if not intent_scores:
            # Ph√¢n t√≠ch d·ª±a tr√™n c·∫•u tr√∫c c√¢u
            if "?" in text or text.endswith("kh√¥ng"):
                intent_scores["question"] = 0.6
            elif any(word in text for word in ["l√†m", "t·∫°o", "gi√∫p", "m·ªü"]):
                intent_scores["command"] = 0.6
            else:
                intent_scores["unknown"] = 0.8
            
        return intent_scores
    
    def extract_enhanced_entities(self, text: str) -> Dict[str, List[str]]:
        """Tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ v·ªõi post-processing"""
        entities = {}
        
        for entity_type, patterns in self.entity_patterns.items():
            matches = []
            for pattern in patterns:
                for match in re.finditer(pattern, text, re.IGNORECASE):
                    entity_text = match.group(0).strip()
                    # Post-processing: filter out short or invalid entities
                    if len(entity_text) > 1 and entity_text not in ["c·ªßa", "trong", "v·ªõi", "v√†", "l√†"]:
                        matches.append(entity_text)
            
            if matches:
                # Remove duplicates while preserving order
                entities[entity_type] = list(dict.fromkeys(matches))
        
        return entities
    
    def analyze_enhanced_sentiment(self, text: str) -> Dict[str, float]:
        """Ph√¢n t√≠ch c·∫£m x√∫c c·∫£i ti·∫øn v·ªõi context awareness"""
        words = re.findall(r'\b\w+\b', text.lower())
        phrases = []
        
        # T·∫°o c√°c c·ª•m t·ª´ 2-3 t·ª´ li√™n ti·∫øp
        for i in range(len(words) - 1):
            phrases.append(words[i] + " " + words[i+1])
            if i < len(words) - 2:
                phrases.append(words[i] + " " + words[i+1] + " " + words[i+2])
        
        # T√≠nh ƒëi·ªÉm c·∫£m x√∫c v·ªõi weight kh√°c nhau
        score = 0.0
        count = 0
        confidence = 0.0
        
        # Ki·ªÉm tra c·ª•m t·ª´ tr∆∞·ªõc (weight cao h∆°n)
        for phrase in phrases:
            if phrase in self.sentiment_words:
                weight = 1.5  # C·ª•m t·ª´ c√≥ tr·ªçng s·ªë cao h∆°n
                score += self.sentiment_words[phrase] * weight
                confidence += abs(self.sentiment_words[phrase]) * weight
                count += weight
        
        # Ki·ªÉm tra t·ª´ng t·ª´
        for word in words:
            if word in self.sentiment_words:
                score += self.sentiment_words[word]
                confidence += abs(self.sentiment_words[word])
                count += 1
        
        # X·ª≠ l√Ω negation (ph·ªß ƒë·ªãnh)
        negation_words = ["kh√¥ng", "ch·∫≥ng", "ch·∫£", "ƒë√¢u", "not", "no", "never"]
        has_negation = any(neg in words for neg in negation_words)
        if has_negation and score != 0:
            score *= -0.8  # ƒê·∫£o ng∆∞·ª£c v√† gi·∫£m intensity
        
        # X·ª≠ l√Ω intensifiers (t·ª´ nh·∫•n m·∫°nh)
        intensifiers = {"r·∫•t": 1.3, "c·ª±c": 1.5, "v√¥ c√πng": 1.4, "very": 1.3, "really": 1.2, "extremely": 1.5}
        for intensifier, multiplier in intensifiers.items():
            if intensifier in words and score != 0:
                score *= multiplier
                confidence *= multiplier
                break
        
        # T√≠nh to√°n k·∫øt qu·∫£
        if count > 0:
            avg_score = score / count
            confidence = min(confidence / count, 1.0)
        else:
            avg_score = 0.0
            confidence = 0.0
            
        sentiment = {
            "score": avg_score,
            "confidence": confidence,
            "label": "positive" if avg_score > 0.15 else "negative" if avg_score < -0.15 else "neutral"
        }
        
        return sentiment
    
    def extract_smart_keywords(self, text: str) -> List[str]:
        """Tr√≠ch xu·∫•t t·ª´ kh√≥a th√¥ng minh v·ªõi frequency analysis"""
        # Chu·∫©n h√≥a text
        text = re.sub(r'[^\w\s\u00C0-\u024F\u1E00-\u1EFF]', ' ', text.lower())
        words = text.split()
        
        # Extended stopwords list
        stopwords = {
            "v√†", "ho·∫∑c", "l√†", "c·ªßa", "t·ª´", "v·ªõi", "c√°c", "nh·ªØng", "m·ªôt", "c√≥", "kh√¥ng", 
            "ƒë∆∞·ª£c", "trong", "ƒë·∫øn", "cho", "v·ªÅ", "ƒë·ªÉ", "theo", "t·∫°i", "b·ªüi", "v√¨", "n·∫øu", 
            "khi", "m√†", "nh∆∞", "th√¨", "nh∆∞ng", "t√¥i", "b·∫°n", "anh", "ch·ªã", "h·ªç", "ch√∫ng", 
            "m√¨nh", "n√†y", "ƒë√≥", "ƒë√¢y", "kia", "th·∫ø", "v·∫≠y", "r·ªìi", "s·∫Ω", "ƒë√£", "ƒëang",
            "the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for", "of", 
            "with", "by", "from", "up", "about", "into", "through", "during", "before", 
            "after", "above", "below", "between", "among", "is", "are", "was", "were", 
            "be", "been", "being", "have", "has", "had", "do", "does", "did", "will", 
            "would", "could", "should", "may", "might", "must", "can", "shall"
        }
        
        # Filter words v√† t√≠nh frequency
        filtered_words = [word for word in words if word not in stopwords and len(word) > 2]
        word_freq = Counter(filtered_words)
        
        # L·∫•y top keywords based on frequency v√† length
        keywords = []
        for word, freq in word_freq.most_common():
            # Bonus cho t·ª´ d√†i h∆°n v√† frequency cao
            score = freq * (len(word) / 5.0)
            if score >= 1.0:  # Threshold
                keywords.append(word)
            
            if len(keywords) >= 10:  # Limit s·ªë keywords
                break
        
        return keywords
    
    def _get_main_intent(self, analysis: Dict[str, Any]) -> str:
        """X√°c ƒë·ªãnh √Ω ƒë·ªãnh ch√≠nh t·ª´ analysis results"""
        intents = analysis.get("intent", {})
        if not intents:
            return "unknown"
        
        # S·∫Øp x·∫øp theo score v√† l·∫•y intent cao nh·∫•t
        sorted_intents = sorted(intents.items(), key=lambda x: x[1], reverse=True)
        return sorted_intents[0][0]
    
    def _is_reminder_related(self, command: str, analysis: Dict[str, Any]) -> bool:
        """Ki·ªÉm tra xem l·ªánh c√≥ li√™n quan ƒë·∫øn reminder hay kh√¥ng"""
        # Ki·ªÉm tra t·ª´ kh√≥a tr·ª±c ti·∫øp
        reminder_keywords = ["nh·∫Øc", "nh·∫Øc nh·ªü", "reminder", "ghi ch√∫", "l·ªãch", "schedule", "calendar"]
        if any(keyword in command.lower() for keyword in reminder_keywords):
            return True
        
        # Ki·ªÉm tra intent
        main_intent = self._get_main_intent(analysis)
        if main_intent in ["reminder", "delete_reminder", "list_reminder"]:
            return True
        
        # Ki·ªÉm tra reminder_action
        reminder_action = analysis.get("reminder_action", {})
        if reminder_action.get("action_type"):
            return True
        
        return False
    
    def _update_context(self, command: str, analysis: Dict[str, Any]):
        """C·∫≠p nh·∫≠t context memory"""
        context_entry = {
            "command": command,
            "intent": self._get_main_intent(analysis),
            "entities": analysis.get("entities", {}),
            "timestamp": datetime.datetime.now().isoformat()
        }
        
        self.context_memory.append(context_entry)
        
        # Gi·ªØ ch·ªâ 10 context entries g·∫ßn nh·∫•t
        if len(self.context_memory) > 10:
            self.context_memory.pop(0)
    
    def _calculate_context_score(self, text: str, analysis: Dict[str, Any]) -> float:
        """T√≠nh ƒëi·ªÉm relevance d·ª±a tr√™n context"""
        if not self.context_memory:
            return 0.0
        
        # Ki·ªÉm tra similarity v·ªõi c√°c l·ªánh tr∆∞·ªõc ƒë√≥
        current_keywords = set(analysis.get("keywords", []))
        total_score = 0.0
        
        for context in self.context_memory[-3:]:  # Ch·ªâ x√©t 3 context g·∫ßn nh·∫•t
            context_keywords = set()
            for entity_list in context.get("entities", {}).values():
                context_keywords.update(entity_list)
            
            # T√≠nh Jaccard similarity
            if current_keywords or context_keywords:
                intersection = len(current_keywords.intersection(context_keywords))
                union = len(current_keywords.union(context_keywords))
                similarity = intersection / union if union > 0 else 0
                total_score += similarity
        
        return min(total_score / 3, 1.0) if self.context_memory else 0.0
    
    def _assess_complexity(self, text: str, analysis: Dict[str, Any]) -> str:
        """ƒê√°nh gi√° ƒë·ªô ph·ª©c t·∫°p c·ªßa command"""
        word_count = len(text.split())
        entity_count = sum(len(entities) for entities in analysis.get("entities", {}).values())
        intent_count = len(analysis.get("intent", {}))
        
        complexity_score = word_count * 0.3 + entity_count * 0.5 + intent_count * 0.2
        
        if complexity_score < 3:
            return "simple"
        elif complexity_score < 7:
            return "medium"
        else:
            return "complex"
    
    def _calculate_confidence(self, analysis: Dict[str, Any]) -> float:
        """T√≠nh confidence score cho analysis"""
        intents = analysis.get("intent", {})
        entities = analysis.get("entities", {})
        sentiment = analysis.get("sentiment", {})
        
        # Intent confidence
        intent_conf = max(intents.values()) if intents else 0.0
        
        # Entity confidence (based on count)
        entity_conf = min(len(entities) * 0.2, 1.0)
        
        # Sentiment confidence
        sentiment_conf = sentiment.get("confidence", 0.0) if sentiment else 0.0
        
        # Weighted average
        overall_confidence = (intent_conf * 0.5 + entity_conf * 0.3 + sentiment_conf * 0.2)
        
        return min(overall_confidence, 1.0)
    
    def _is_question(self, analysis: Dict[str, Any]) -> bool:
        """Ki·ªÉm tra xem ph√¢n t√≠ch c√≥ bi·ªÉu th·ªã m·ªôt c√¢u h·ªèi hay kh√¥ng"""
        # Ki·ªÉm tra intent
        intents = analysis.get("intent", {})
        if "question" in intents and intents["question"] > 0.5:
            return True
        
        # Ki·ªÉm tra c√°c t·ª´ kh√≥a c√¢u h·ªèi ƒë·∫∑c tr∆∞ng trong vƒÉn b·∫£n chu·∫©n h√≥a
        text = analysis.get("normalized_text", "").lower()
        question_indicators = ["l√† g√¨", "l√† ai", "·ªü ƒë√¢u", "bao nhi√™u", "th·∫ø n√†o", "t·∫°i sao", "khi n√†o", "ai l√†", "?", "g√¨", "n√†o"]
        return any(indicator in text for indicator in question_indicators)
    
    def _extract_search_query(self, command: str, analysis: Dict[str, Any]) -> str:
        """Tr√≠ch xu·∫•t truy v·∫•n t√¨m ki·∫øm t·ª´ l·ªánh v√† ph√¢n t√≠ch"""
        # L·∫•y vƒÉn b·∫£n ƒë√£ chu·∫©n h√≥a
        normalized_text = analysis.get("normalized_text", command)
        
        # V·ªõi c√°c c√¢u h·ªèi, th∆∞·ªùng t·ªët nh·∫•t l√† gi·ªØ nguy√™n c·∫•u tr√∫c c√¢u h·ªèi nh∆∞ng lo·∫°i b·ªè m·ªôt s·ªë t·ª´ kh√¥ng c·∫ßn thi·∫øt
        if self._is_question(analysis):
            # Lo·∫°i b·ªè c√°c t·ª´ kh√¥ng c·∫ßn thi·∫øt cho t√¨m ki·∫øm nh∆∞ng gi·ªØ c·∫•u tr√∫c c√¢u h·ªèi
            stop_words = ["t√¥i", "b·∫°n", "m√¨nh", "tao", "t·ªõ", "ch√∫ng ta", "h·ªç", "c√°c b·∫°n"]
            query_words = [word for word in normalized_text.split() if word not in stop_words]
            query = " ".join(query_words)
            
            # ƒê·∫£m b·∫£o truy v·∫•n kh√¥ng qu√° d√†i
            if len(query) > 100:
                query = query[:100]
                
            return query.strip()
        
        # N·∫øu kh√¥ng ph·∫£i c√¢u h·ªèi, ti·∫øp t·ª•c v·ªõi logic c≈©
        entities = analysis.get("entities", {})
        keywords = analysis.get("keywords", [])
        
        # ∆Øu ti√™n c√°c lo·∫°i th·ª±c th·ªÉ quan tr·ªçng cho t√¨m ki·∫øm
        priority_entities = ["person", "location", "date", "time"]
        search_terms = []
        
        # Th√™m c√°c th·ª±c th·ªÉ ∆∞u ti√™n v√†o truy v·∫•n
        for entity_type in priority_entities:
            if entity_type in entities:
                search_terms.extend(entities[entity_type])
         
        # Th√™m c√°c t·ª´ kh√≥a v√†o truy v·∫•n
        search_terms.extend(keywords[:5])  # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng t·ª´ kh√≥a
         
        # Lo·∫°i b·ªè c√°c t·ª´ tr√πng l·∫∑p v√† gi·ªØ l·∫°i t·ªëi ƒëa 7 t·ª´ (ƒë·ªô d√†i t·ªëi ∆∞u cho t√¨m ki·∫øm)
        unique_terms = list(dict.fromkeys(search_terms))  # Lo·∫°i b·ªè tr√πng l·∫∑p nh∆∞ng gi·ªØ th·ª© t·ª±
        query_terms = unique_terms[:7]
         
        # T·∫°o truy v·∫•n t·ª´ c√°c t·ª´ ƒë√£ ch·ªçn
        if query_terms:
            query = " ".join(query_terms)
        else:
            # N·∫øu kh√¥ng c√≥ t·ª´ n√†o, s·ª≠ d·ª•ng vƒÉn b·∫£n chu·∫©n h√≥a
            query = normalized_text
        
        # Lo·∫°i b·ªè c√°c t·ª´ kh√¥ng c·∫ßn thi·∫øt cho t√¨m ki·∫øm
        stop_words = ["l√†", "c·ªßa", "trong", "v·ªõi", "v√†", "ho·∫∑c", "c√≥", "ƒë∆∞·ª£c", "b·ªüi", "t·∫°i", "v√¨", "n·∫øu", "khi", "m√†", "nh∆∞", "th√¨", "nh∆∞ng"]
        query_words = [word for word in query.split() if word not in stop_words]
        query = " ".join(query_words)
        
        # N·∫øu truy v·∫•n qu√° ng·∫Øn, s·ª≠ d·ª•ng l·∫°i l·ªánh g·ªëc (ƒë√£ chu·∫©n h√≥a)
        if len(query) < 3:
            # Lo·∫°i b·ªè c√°c t·ª´ kh√¥ng c·∫ßn thi·∫øt kh·ªèi l·ªánh g·ªëc
            command_words = [word for word in normalized_text.split() if word not in stop_words]
            query = " ".join(command_words)
             
        # ƒê·∫£m b·∫£o truy v·∫•n kh√¥ng qu√° d√†i
        if len(query) > 100:
            query = query[:100]
            
        return query.strip()
    
    def _format_analysis_result(self, analysis: Dict[str, Any]) -> str:
        """Format detailed analysis results"""
        result = "üß† K·∫øt qu·∫£ ph√¢n t√≠ch ng√¥n ng·ªØ c·∫£i ti·∫øn:\n\n"
        
        # Intent v·ªõi confidence
        intents = analysis.get("intent", {})
        if intents:
            result += "üéØ √ù ƒë·ªãnh: "
            sorted_intents = sorted(intents.items(), key=lambda x: x[1], reverse=True)
            intent_name = sorted_intents[0][0]
            intent_score = sorted_intents[0][1]
            
            intent_map = {
                "question": "C√¢u h·ªèi",
                "command": "Y√™u c·∫ßu/M·ªánh l·ªánh",
                "reminder": "T·∫°o nh·∫Øc nh·ªü",
                "delete_reminder": "X√≥a nh·∫Øc nh·ªü",
                "list_reminder": "Xem nh·∫Øc nh·ªü",
                "greeting": "L·ªùi ch√†o",
                "farewell": "L·ªùi t·∫°m bi·ªát",
                "thanks": "L·ªùi c·∫£m ∆°n",
                "apology": "L·ªùi xin l·ªói",
                "unknown": "Kh√¥ng x√°c ƒë·ªãnh"
            }
            result += f"{intent_map.get(intent_name, intent_name)} ({intent_score:.2f})\n\n"
        
        # Sentiment v·ªõi confidence
        sentiment = analysis.get("sentiment", {})
        if sentiment:
            result += "üòä C·∫£m x√∫c: "
            sentiment_map = {
                "positive": "T√≠ch c·ª±c",
                "negative": "Ti√™u c·ª±c", 
                "neutral": "Trung t√≠nh"
            }
            label = sentiment_map.get(sentiment.get('label'), sentiment.get('label', 'Trung t√≠nh'))
            score = sentiment.get('score', 0)
            confidence = sentiment.get('confidence', 0)
            result += f"{label} (ƒëi·ªÉm: {score:.2f}, tin c·∫≠y: {confidence:.2f})\n\n"
        
        # Entities
        entities = analysis.get("entities", {})
        if entities:
            result += "üè∑Ô∏è Th·ª±c th·ªÉ:\n"
            entity_map = {
                "date": "Ng√†y th√°ng",
                "time": "Th·ªùi gian",
                "person": "Ng∆∞·ªùi",
                "location": "ƒê·ªãa ƒëi·ªÉm",
                "number": "S·ªë",
                "priority": "∆Øu ti√™n",
                "duration": "Th·ªùi l∆∞·ª£ng"
            }
            for entity_type, entity_list in entities.items():
                result += f"  - {entity_map.get(entity_type, entity_type)}: {', '.join(entity_list)}\n"
            result += "\n"
        
        # Keywords
        keywords = analysis.get("keywords", [])
        if keywords:
            result += f"üîë T·ª´ kh√≥a: {', '.join(keywords[:8])}\n\n"  # Limit to 8 keywords
        
        # Advanced metrics
        context_score = analysis.get("context_score", 0)
        complexity = analysis.get("complexity", "unknown")
        confidence = analysis.get("confidence", 0)
        
        result += f"üìä Ph√¢n t√≠ch n√¢ng cao:\n"
        result += f"  - ƒêi·ªÉm ng·ªØ c·∫£nh: {context_score:.2f}\n"
        result += f"  - ƒê·ªô ph·ª©c t·∫°p: {complexity}\n"
        result += f"  - ƒê·ªô tin c·∫≠y t·ªïng th·ªÉ: {confidence:.2f}\n\n"
        
        # Normalized text
        result += f"üìù VƒÉn b·∫£n chu·∫©n h√≥a: {analysis.get('normalized_text', '')}"
        
        return result
        
    def analyze_reminder_action(self, text: str) -> Dict[str, Any]:
        """Ph√¢n t√≠ch h√†nh ƒë·ªông li√™n quan ƒë·∫øn nh·∫Øc nh·ªü v√† ghi ch√∫"""
        result = {
            "action_type": None,
            "reminder_id": None,
            "reminder_content": None,
            "time_info": None
        }
        
        # X√°c ƒë·ªãnh lo·∫°i h√†nh ƒë·ªông
        if any(word in text for word in ["x√≥a", "h·ªßy", "lo·∫°i b·ªè", "b·ªè", "xo√°", "hu·ª∑", "g·ª° b·ªè"]):
            result["action_type"] = "delete"
        elif any(word in text for word in ["th√™m", "t·∫°o", "ƒë·∫∑t", "l√™n l·ªãch", "nh·∫Øc"]):
            result["action_type"] = "add"
        elif any(word in text for word in ["xem", "hi·ªÉn th·ªã", "li·ªát k√™", "ki·ªÉm tra", "c√≥ g√¨", "c√≥ nh·ªØng"]):
            result["action_type"] = "list"
        elif any(word in text for word in ["c·∫≠p nh·∫≠t", "s·ª≠a", "ch·ªânh s·ª≠a", "thay ƒë·ªïi"]):
            result["action_type"] = "update"
        
        # T√¨m ID nh·∫Øc nh·ªü
        id_match = re.search(r'id[:\s]*(\d+)', text, re.IGNORECASE)
        if id_match:
            result["reminder_id"] = id_match.group(1)
        
        # T√¨m n·ªôi dung nh·∫Øc nh·ªü (sau c√°c t·ª´ kh√≥a h√†nh ƒë·ªông)
        if result["action_type"] in ["delete", "update"]:
            action_keywords = {
                "delete": ["x√≥a", "h·ªßy", "lo·∫°i b·ªè", "b·ªè", "xo√°", "hu·ª∑", "g·ª° b·ªè"],
                "update": ["c·∫≠p nh·∫≠t", "s·ª≠a", "ch·ªânh s·ª≠a", "thay ƒë·ªïi"]
            }
            
            for keyword in action_keywords[result["action_type"]]:
                if keyword in text:
                    parts = text.split(keyword, 1)
                    if len(parts) > 1:
                        content = parts[1].strip()
                        # Lo·∫°i b·ªè c√°c t·ª´ kh√¥ng c·∫ßn thi·∫øt
                        for word in ["nh·∫Øc nh·ªü", "ghi ch√∫", "l·ªãch", "s·ª± ki·ªán", "gi√∫p", "t√¥i", "cho", "c·ªßa"]:
                            content = content.replace(word, "").strip()
                        
                        if content:
                            result["reminder_content"] = content
                            break
        
        # T√¨m th√¥ng tin th·ªùi gian
        time_entities = self.extract_entities(text).get("date", []) + self.extract_entities(text).get("time", [])
        if time_entities:
            result["time_info"] = time_entities
        
        return result
    
    def detect_intent(self, text: str) -> Dict[str, float]:
        """Ph√°t hi·ªán √Ω ƒë·ªãnh t·ª´ vƒÉn b·∫£n"""
        intent_scores = {}
        
        # Ki·ªÉm tra c√°c pattern v·ªõi tr·ªçng s·ªë kh√°c nhau
        for intent, patterns in self.intent_patterns.items():
            max_score = 0.0
            pattern_matches = 0
            
            for pattern in patterns:
                if re.search(pattern, text):
                    pattern_matches += 1
                    score = 0.5 + (pattern_matches * 0.2)  # TƒÉng ƒëi·ªÉm theo s·ªë pattern kh·ªõp
                    max_score = max(max_score, min(score, 1.0))
            
            if max_score > 0:
                intent_scores[intent] = max_score
        
        # X·ª≠ l√Ω ƒë·∫∑c bi·ªát cho c√°c intent ph·ª©c t·∫°p
        if any(word in text for word in ["x√≥a", "h·ªßy", "delete", "remove"]) and any(word in text for word in ["nh·∫Øc", "ghi ch√∫", "l·ªãch", "reminder"]):
            intent_scores["delete_reminder"] = 0.9
        
        if any(word in text for word in ["xem", "hi·ªÉn th·ªã", "list", "show"]) and any(word in text for word in ["nh·∫Øc", "ghi ch√∫", "l·ªãch", "reminder"]):
            intent_scores["list_reminder"] = 0.9
            
        # N·∫øu kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c √Ω ƒë·ªãnh r√µ r√†ng
        if not intent_scores:
            intent_scores["unknown"] = 1.0
            
        return intent_scores
    
    def extract_entities(self, text: str) -> Dict[str, List[str]]:
        """Tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ t·ª´ vƒÉn b·∫£n"""
        entities = {}
        
        for entity_type, patterns in self.entity_patterns.items():
            matches = []
            for pattern in patterns:
                for match in re.finditer(pattern, text):
                    matches.append(match.group(0))
            
            if matches:
                entities[entity_type] = matches
        
        return entities
    
    def analyze_sentiment(self, text: str) -> Dict[str, float]:
        """Ph√¢n t√≠ch c·∫£m x√∫c trong vƒÉn b·∫£n"""
        words = re.findall(r'\b\w+\b', text)
        phrases = []
        
        # T·∫°o c√°c c·ª•m t·ª´ 2 t·ª´ li√™n ti·∫øp
        for i in range(len(words) - 1):
            phrases.append(words[i] + " " + words[i+1])
        
        # T√≠nh ƒëi·ªÉm c·∫£m x√∫c
        score = 0.0
        count = 0
        
        # Ki·ªÉm tra c·ª•m t·ª´ tr∆∞·ªõc
        for phrase in phrases:
            if phrase in self.sentiment_words:
                score += self.sentiment_words[phrase]
                count += 1
        
        # Ki·ªÉm tra t·ª´ng t·ª´
        for word in words:
            if word in self.sentiment_words:
                score += self.sentiment_words[word]
                count += 1
        
        # X√°c ƒë·ªãnh c·∫£m x√∫c
        if count > 0:
            avg_score = score / count
        else:
            avg_score = 0.0
            
        sentiment = {
            "score": avg_score,
            "label": "positive" if avg_score > 0.1 else "negative" if avg_score < -0.1 else "neutral"
        }
        
        return sentiment
    
    def extract_keywords(self, text: str) -> List[str]:
        """Tr√≠ch xu·∫•t t·ª´ kh√≥a quan tr·ªçng t·ª´ vƒÉn b·∫£n"""
        # Lo·∫°i b·ªè d·∫•u c√¢u
        text = text.translate(str.maketrans("", "", string.punctuation))
        
        # T√°ch t·ª´
        words = text.split()
        
        # Lo·∫°i b·ªè c√°c t·ª´ d·ª´ng (stopwords)
        stopwords = ["v√†", "ho·∫∑c", "l√†", "c·ªßa", "t·ª´", "v·ªõi", "c√°c", "nh·ªØng", "m·ªôt", "c√≥", "kh√¥ng", 
                    "ƒë∆∞·ª£c", "trong", "ƒë·∫øn", "cho", "v·ªÅ", "ƒë·ªÉ", "theo", "t·∫°i", "b·ªüi", "v√¨", "n·∫øu", 
                    "khi", "m√†", "nh∆∞", "th√¨", "nh∆∞ng", "t√¥i", "b·∫°n", "anh", "ch·ªã", "h·ªç", "ch√∫ng", 
                    "m√¨nh", "n√†y", "ƒë√≥", "ƒë√¢y", "kia", "th·∫ø", "v·∫≠y"]
        
        keywords = [word for word in words if word not in stopwords and len(word) > 1]
        
        # Tr·∫£ v·ªÅ danh s√°ch t·ª´ kh√≥a duy nh·∫•t
        return list(set(keywords))
    
    def normalize_text(self, text: str) -> str:
        """Chu·∫©n h√≥a vƒÉn b·∫£n"""
        # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
        text = re.sub(r'\s+', ' ', text).strip()
        
        # Chu·∫©n h√≥a d·∫•u c√¢u
        text = re.sub(r'\s*([,.!?;:])\s*', r'\1 ', text)
        
        # Chu·∫©n h√≥a ch·ªØ hoa ƒë·∫ßu c√¢u
        sentences = re.split(r'([.!?]\s)', text)
        normalized_text = ""
        
        for i in range(0, len(sentences), 2):
            if i < len(sentences):
                sentence = sentences[i].strip()
                if sentence:
                    sentence = sentence[0].upper() + sentence[1:]
                    normalized_text += sentence
                
                if i + 1 < len(sentences):
                    normalized_text += sentences[i + 1]
        
        return normalized_text

# Singleton instance
_nlp_processor = None

def get_nlp_processor() -> EnhancedNLPProcessor:
    """Tr·∫£ v·ªÅ instance c·ªßa EnhancedNLPProcessor (singleton)"""
    global _nlp_processor
    if _nlp_processor is None:
        _nlp_processor = EnhancedNLPProcessor()
    return _nlp_processor

def analyze_user_input(text: str) -> Dict[str, Any]:
    """Ph√¢n t√≠ch ƒë·∫ßu v√†o c·ªßa ng∆∞·ªùi d√πng v·ªõi enhanced capabilities"""
    processor = get_nlp_processor()
    return processor.analyze_text_with_context(text)

def enhance_with_nlp(command: str) -> str:
    """H√†m ch√≠nh ƒë·ªÉ x·ª≠ l√Ω l·ªánh v·ªõi NLP"""
    if not command:
        return "Vui l√≤ng cung c·∫•p vƒÉn b·∫£n ƒë·ªÉ ph√¢n t√≠ch."
    
    # Ph√¢n t√≠ch vƒÉn b·∫£n
    analysis = analyze_user_input(command)
    
    # T·∫°o k·∫øt qu·∫£ ph√¢n t√≠ch
    result = "üìä K·∫øt qu·∫£ ph√¢n t√≠ch ng√¥n ng·ªØ:\n\n"
    
    # √ù ƒë·ªãnh
    result += "üéØ √ù ƒë·ªãnh: "
    intents = sorted(analysis["intent"].items(), key=lambda x: x[1], reverse=True)
    if intents:
        intent_name = intents[0][0]
        intent_map = {
            "question": "C√¢u h·ªèi",
            "command": "Y√™u c·∫ßu/M·ªánh l·ªánh",
            "greeting": "L·ªùi ch√†o",
            "farewell": "L·ªùi t·∫°m bi·ªát",
            "thanks": "L·ªùi c·∫£m ∆°n",
            "apology": "L·ªùi xin l·ªói",
            "unknown": "Kh√¥ng x√°c ƒë·ªãnh"
        }
        result += f"{intent_map.get(intent_name, intent_name)} ({intents[0][1]:.2f})\n"
    else:
        result += "Kh√¥ng x√°c ƒë·ªãnh\n"
    
    # C·∫£m x√∫c
    sentiment = analysis["sentiment"]
    result += "üòä C·∫£m x√∫c: "
    sentiment_map = {
        "positive": "T√≠ch c·ª±c",
        "negative": "Ti√™u c·ª±c",
        "neutral": "Trung t√≠nh"
    }
    result += f"{sentiment_map.get(sentiment['label'], sentiment['label'])} ({sentiment['score']:.2f})\n"
    
    # Th·ª±c th·ªÉ
    if analysis["entities"]:
        result += "üè∑Ô∏è Th·ª±c th·ªÉ:\n"
        entity_map = {
            "date": "Ng√†y th√°ng",
            "time": "Th·ªùi gian",
            "person": "Ng∆∞·ªùi",
            "location": "ƒê·ªãa ƒëi·ªÉm",
            "number": "S·ªë"
        }
        for entity_type, entities in analysis["entities"].items():
            result += f"  - {entity_map.get(entity_type, entity_type)}: {', '.join(entities)}\n"
    
    # T·ª´ kh√≥a
    if analysis["keywords"]:
        result += "üîë T·ª´ kh√≥a: " + ", ".join(analysis["keywords"]) + "\n"
    
    # VƒÉn b·∫£n chu·∫©n h√≥a
    result += "üìù VƒÉn b·∫£n chu·∫©n h√≥a: " + analysis["normalized_text"]
    
    return result