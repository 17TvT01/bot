import re
import string
import datetime
from typing import Dict, List, Tuple, Optional, Any
from collections import Counter, defaultdict
import json

# Tá»« khÃ³a vÃ  máº«u cho viá»‡c nháº­n diá»‡n tÃ­nh nÄƒng
keywords = ["hiá»ƒu", "phÃ¢n tÃ­ch", "ngÃ´n ngá»¯", "nlp", "xá»­ lÃ½", "lá»i nÃ³i", "cáº£m xÃºc", "Ã½ Ä‘á»‹nh"]

patterns = [
    "hiá»ƒu lá»i nÃ³i",
    "phÃ¢n tÃ­ch ngÃ´n ngá»¯",
    "xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn",
    "hiá»ƒu Ã½ Ä‘á»‹nh",
    "phÃ¢n tÃ­ch cáº£m xÃºc",
    "xÃ³a ghi chÃº",
    "xÃ³a nháº¯c nhá»Ÿ",
    "há»§y lá»‹ch"
]

class EnhancedNLPProcessor:
    """Bá»™ xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn cáº£i tiáº¿n vá»›i kháº£ nÄƒng hiá»ƒu ngá»¯ cáº£nh"""
    
    def __init__(self):
        self.intent_patterns = self._load_enhanced_intent_patterns()
        self.entity_patterns = self._load_enhanced_entity_patterns()
        self.sentiment_words = self._load_enhanced_sentiment_words()
        self.synonyms = self._load_synonyms()
        self.context_memory = []  # LÆ°u trá»¯ ngá»¯ cáº£nh há»™i thoáº¡i
        
    def process_command(self, command: str) -> str:
        """Xá»­ lÃ½ lá»‡nh tá»« ngÆ°á»i dÃ¹ng vá»›i enhanced processing"""
        if not command:
            return "TÃ´i cÃ³ thá»ƒ giÃºp phÃ¢n tÃ­ch ngÃ´n ngá»¯, nháº­n diá»‡n Ã½ Ä‘á»‹nh, vÃ  xá»­ lÃ½ cÃ¡c lá»‡nh liÃªn quan Ä‘áº¿n nháº¯c nhá»Ÿ vá»›i kháº£ nÄƒng hiá»ƒu ngá»¯ cáº£nh tá»‘t hÆ¡n."
        
        # Enhance command vá»›i synonyms vÃ  normalization
        enhanced_command = self._enhance_command(command)
        
        # PhÃ¢n tÃ­ch vÄƒn báº£n vá»›i context
        analysis = self.analyze_text_with_context(enhanced_command)
        
        # XÃ¡c Ä‘á»‹nh Ã½ Ä‘á»‹nh chÃ­nh
        main_intent = self._get_main_intent(analysis)
        
        # Xá»­ lÃ½ lá»‡nh liÃªn quan Ä‘áº¿n nháº¯c nhá»Ÿ
        if self._is_reminder_related(enhanced_command, analysis):
            from features.reminder import reminder
            return reminder(enhanced_command)
        
        # Cáº­p nháº­t context memory
        self._update_context(enhanced_command, analysis)
        
        # Hiá»ƒn thá»‹ káº¿t quáº£ phÃ¢n tÃ­ch cáº£i tiáº¿n
        return self._format_analysis_result(analysis)
    
    def _enhance_command(self, command: str) -> str:
        """Cáº£i thiá»‡n lá»‡nh báº±ng cÃ¡ch thay tháº¿ synonyms vÃ  normalize"""
        enhanced = command.lower().strip()
        
        # Thay tháº¿ synonyms
        for main_word, synonyms in self.synonyms.items():
            for synonym in synonyms:
                enhanced = enhanced.replace(synonym, main_word)
        
        # Normalize dáº¥u cÃ¢u vÃ  khoáº£ng tráº¯ng
        enhanced = re.sub(r'\s+', ' ', enhanced)
        enhanced = re.sub(r'[^\w\s\u00C0-\u024F\u1E00-\u1EFF]', ' ', enhanced)
        
        return enhanced.strip()
    
    def analyze_text_with_context(self, text: str) -> Dict[str, Any]:
        """PhÃ¢n tÃ­ch vÄƒn báº£n vá»›i context awareness"""
        # PhÃ¢n tÃ­ch cÆ¡ báº£n
        basic_analysis = {
            "intent": self.detect_enhanced_intent(text),
            "entities": self.extract_enhanced_entities(text),
            "sentiment": self.analyze_enhanced_sentiment(text),
            "keywords": self.extract_smart_keywords(text),
            "normalized_text": self.normalize_text(text),
            "reminder_action": self.analyze_reminder_action(text)
        }
        
        # ThÃªm context analysis
        context_score = self._calculate_context_score(text, basic_analysis)
        complexity = self._assess_complexity(text, basic_analysis)
        confidence = self._calculate_confidence(basic_analysis)
        
        enhanced_analysis = {
            **basic_analysis,
            "context_score": context_score,
            "complexity": complexity,
            "confidence": confidence,
            "enhanced": True
        }
        
        return enhanced_analysis
        
    def _load_enhanced_intent_patterns(self) -> Dict[str, List[str]]:
        """Táº£i cÃ¡c máº«u nháº­n diá»‡n Ã½ Ä‘á»‹nh Ä‘Æ°á»£c cáº£i tiáº¿n"""
        return {
            "question": [
                r"^(?:ai|báº¡n|trá»£ lÃ½)\s+(?:cÃ³ thá»ƒ|biáº¿t|cho\s+(?:tÃ´i|mÃ¬nh|tao|tá»›)\s+(?:biáº¿t|xem))\s+",
                r"^(?:tÃ´i|mÃ¬nh|tao|tá»›)\s+(?:muá»‘n|cáº§n|thÃ­ch)\s+(?:biáº¿t|xem|tÃ¬m)\s+",
                r"(?:lÃ \s+(?:gÃ¬|sao|tháº¿ nÃ o))",
                r"(?:nhÆ°\s+tháº¿\s+nÃ o)",
                r"(?:á»Ÿ\s+Ä‘Ã¢u)",
                r"(?:khi\s+nÃ o)",
                r"(?:ai|ngÆ°á»i nÃ o)\s+(?:lÃ |Ä‘Ã£|sáº½)",
                r"(?:táº¡i\s+sao|vÃ¬\s+sao|táº¡i\s+vÃ¬|vÃ¬|bá»Ÿi\s+vÃ¬)",
                r"(?:tháº¿\s+nÃ o|ra\s+sao|nhÆ°\s+tháº¿\s+nÃ o)",
                r"(?:cÃ³|khÃ´ng)\s+(?:pháº£i|Ä‘Ãºng|tháº­t)\s+(?:lÃ |khÃ´ng)",
                r"(?:máº¥y|bao\s+nhiÃªu)\s+(?:giá»|phÃºt|giÃ¢y|ngÃ y|thÃ¡ng|nÄƒm)",
                r"\?\s*$"  # Káº¿t thÃºc báº±ng dáº¥u há»i
            ],
            "command": [
                r"^(?:hÃ£y|vui\s+lÃ²ng)\s+",
                r"^(?:tÃ´i|mÃ¬nh|tao|tá»›)\s+(?:muá»‘n|cáº§n|thÃ­ch)\s+(?:báº¡n|trá»£ lÃ½)\s+",
                r"^(?:giÃºp|giÃºp\s+(?:tÃ´i|mÃ¬nh|tao|tá»›))\s+",
                r"^(?:lÃ m|táº¡o|viáº¿t|tÃ­nh|tÃ¬m|má»Ÿ|Ä‘Ã³ng|lÆ°u|xÃ³a)\s+",
                r"^(?:cho\s+(?:tÃ´i|mÃ¬nh|tao|tá»›)\s+(?:xem|biáº¿t))\s+",
                r"^(?:khá»Ÿi Ä‘á»™ng|cháº¡y|start|run|execute)"
            ],
            "reminder": [
                r"(?:nháº¯c|nháº¯c nhá»Ÿ|remind|reminder)\s+(?:tÃ´i|mÃ¬nh|tao|tá»›)",
                r"(?:Ä‘áº·t|táº¡o|thÃªm)\s+(?:nháº¯c nhá»Ÿ|lá»‹ch|reminder)",
                r"(?:lÃªn lá»‹ch|schedule)\s+",
                r"(?:ghi chÃº|note)\s+(?:vá»|cho|ráº±ng)",
                r"(?:nhá»›|remember)\s+(?:ráº±ng|lÃ |that)"
            ],
            "delete_reminder": [
                r"(?:xÃ³a|há»§y|loáº¡i bá»|delete|remove)\s+(?:nháº¯c nhá»Ÿ|lá»‹ch|reminder|ghi chÃº)",
                r"(?:bá»|gá»¡)\s+(?:nháº¯c nhá»Ÿ|lá»‹ch|reminder)",
                r"(?:huá»·|cancel)\s+(?:lá»‹ch|appointment|meeting)",
                r"(?:khÃ´ng\s+cáº§n|bá» qua)\s+(?:nháº¯c nhá»Ÿ|reminder)"
            ],
            "list_reminder": [
                r"(?:xem|hiá»ƒn thá»‹|show|list)\s+(?:nháº¯c nhá»Ÿ|lá»‹ch|reminder)",
                r"(?:cÃ³\s+(?:gÃ¬|nhá»¯ng\s+gÃ¬|nháº¯c nhá»Ÿ\s+nÃ o))",
                r"(?:danh sÃ¡ch|list)\s+(?:nháº¯c nhá»Ÿ|lá»‹ch|reminder)",
                r"(?:kiá»ƒm tra|check)\s+(?:lá»‹ch|calendar|schedule)"
            ],
            "greeting": [
                r"^(?:xin\s+chÃ o|chÃ o|hello|hi|hey|good\s+morning|good\s+afternoon|good\s+evening)\s*$",
                r"^(?:chÃ o\s+buá»•i\s+(?:sÃ¡ng|trÆ°a|chiá»u|tá»‘i))\s*$"
            ],
            "farewell": [
                r"^(?:táº¡m\s+biá»‡t|goodbye|bye|see\s+you|háº¹n\s+gáº·p\s+láº¡i)\s*$",
                r"^(?:chÃºc\s+(?:ngá»§\s+ngon|ngá»§\s+ngon|ngon\s+giáº¥c))\s*$"
            ],
            "thanks": [
                r"^(?:cáº£m\s+Æ¡n|thank|thanks|thank\s+you|cÃ¡m\s+Æ¡n)\s*",
                r"^(?:cáº£m\s+Æ¡n\s+(?:báº¡n|trá»£ lÃ½|nhiá»u|ráº¥t|vÃ´ cÃ¹ng))\s*"
            ],
            "apology": [
                r"^(?:xin\s+lá»—i|sorry|i'm\s+sorry|i\s+apologize)\s*",
                r"^(?:tÃ´i|mÃ¬nh|tao|tá»›)\s+(?:xin\s+lá»—i)\s*"
            ]
        }
    
    def _load_entity_patterns(self) -> Dict[str, List[str]]:
        """Táº£i cÃ¡c máº«u nháº­n diá»‡n thá»±c thá»ƒ"""
        return {
            "date": [
                r"(?:ngÃ y|hÃ´m)\s+(?:nay|mai|kia|má»‘t)",
                r"(?:tuáº§n|thÃ¡ng|nÄƒm)\s+(?:nÃ y|sau|trÆ°á»›c|tá»›i|qua)",
                r"(?:\d{1,2})[-/](?:\d{1,2})(?:[-/](?:\d{2,4}))?",
                r"(?:thá»©\s+(?:hai|ba|tÆ°|nÄƒm|sÃ¡u|báº£y)|chá»§\s+nháº­t)"
            ],
            "time": [
                r"(?:\d{1,2})\s*(?:giá»|h|:|g)\s*(?:\d{1,2})?\s*(?:phÃºt|p|')?",
                r"(?:sÃ¡ng|trÆ°a|chiá»u|tá»‘i|Ä‘Ãªm|khuya)",
                r"(?:bÃ¢y\s+giá»|hiá»‡n\s+táº¡i|lÃºc\s+nÃ y)"
            ],
            "person": [
                r"(?:anh|chá»‹|cÃ´|chÃº|bÃ¡c|Ã´ng|bÃ )\s+[A-ZÃ€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»†á»‚á»„ÃŒÃá»Šá»ˆÄ¨Ã’Ã“á»Œá»Ã•Ã”á»’á»á»˜á»”á»–Æ á»œá»šá»¢á»á» Ã™Ãšá»¤á»¦Å¨Æ¯á»ªá»¨á»°á»¬á»®á»²Ãá»´á»¶á»¸Ä][a-zÃ Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]*",
                r"(?:Mr\.|Mrs\.|Ms\.|Dr\.)\s+[A-Z][a-z]*"
            ],
            "location": [
                r"(?:táº¡i|á»Ÿ|trong|ngoÃ i|gáº§n|xa)\s+(?:[A-ZÃ€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»†á»‚á»„ÃŒÃÃá»Šá»ˆÄ¨Ã’Ã“á»Œá»Ã•Ã”á»’á»á»˜á»”á»–Æ á»œá»šá»¢á»á» Ã™Ãšá»¤á»¦Å¨Æ¯á»ªá»¨á»°á»¬á»®á»²Ãá»´á»¶á»¸Ä][a-zÃ Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]*)",
                r"(?:thÃ nh\s+phá»‘|tá»‰nh|quáº­n|huyá»‡n|phÆ°á»ng|xÃ£|Ä‘Æ°á»ng|phá»‘)\s+(?:[A-ZÃ€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»†á»‚á»„ÃŒÃÃá»Šá»ˆÄ¨Ã’Ã“á»Œá»Ã•Ã”á»’á»á»˜á»”á»–Æ á»œá»šá»¢á»á» Ã™Ãšá»¤á»¦Å¨Æ¯á»ªá»¨á»°á»¬á»®á»²Ãá»´á»¶á»¸Ä][a-zÃ Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]*)",
                r"(?:vÄƒn phong|office|phÃ²ng\s+há»p|meeting\s+room|há»™i trÆ°á»ng|auditorium)"
            ],
            "number": [
                r"\d+(?:[,.\s]\d+)*(?:\s*(?:triá»‡u|nghÃ¬n|tá»·|trÄƒm|chá»¥c))?",
                r"(?:má»™t|hai|ba|bá»‘n|nÄƒm|sÃ¡u|báº£y|tÃ¡m|chÃ­n|mÆ°á»i)(?:\s+(?:mÆ°á»i|mÆ°Æ¡i|trÄƒm|nghÃ¬n|triá»‡u|tá»·))*",
                r"(?:vÃ i|nhiá»u|Ã­t|Ä‘Ã´i|cáº·p|tÃ¡|chá»¥c)"
            ],
            "priority": [
                r"(?:quan\s+trá»ng|Æ°u\s+tiÃªn|kháº©n\s+cáº¥p|urgent|important|high|medium|low)",
                r"(?:ráº¥t\s+quan\s+trá»ng|cá»±c\s+ká»³\s+quan\s+trá»ng|critical)"
            ],
            "duration": [
                r"(?:\d+)\s+(?:phÃºt|giá»|ngÃ y|tuáº§n|thÃ¡ng|nÄƒm)",
                r"(?:ná»­a|má»™t\s+ná»­a)\s+(?:phÃºt|giá»|ngÃ y|tuáº§n|thÃ¡ng|nÄƒm)",
                r"(?:tá»«|trong\s+vÃ²ng|khoáº£ng|xáº¥p xá»‰)\s+(?:\d+)\s+(?:phÃºt|giá»|ngÃ y)"
            ]
        }
    
    def _load_enhanced_sentiment_words(self) -> Dict[str, float]:
        """Táº£i tá»« Ä‘iá»ƒn cáº£m xÃºc Ä‘Æ°á»£c má»Ÿ rá»™ng"""
        positive = {
            # CÆ¡ báº£n
            "tá»‘t": 0.8, "hay": 0.7, "tuyá»‡t": 0.9, "tuyá»‡t vá»i": 1.0, "xuáº¥t sáº¯c": 1.0,
            "thÃ­ch": 0.7, "yÃªu": 0.9, "thÃº vá»‹": 0.6, "hÃ i lÃ²ng": 0.8, "vui": 0.7,
            "háº¡nh phÃºc": 0.9, "hÃ i hÆ°á»›c": 0.6, "dá»… chá»‹u": 0.5, "thoáº£i mÃ¡i": 0.6,
            "Ä‘áº¹p": 0.7, "xinh": 0.6, "Ä‘Ã¡ng yÃªu": 0.8, "thÃ´ng minh": 0.7,
            "nhanh": 0.6, "hiá»‡u quáº£": 0.7, "chÃ­nh xÃ¡c": 0.8, "Ä‘Ãºng": 0.7,
            # Má»Ÿ rá»™ng
            "hoÃ n háº£o": 1.0, "tuyá»‡t vá»i": 1.0, "áº¥n tÆ°á»£ng": 0.8, "tÃ i giá»i": 0.9,
            "cool": 0.7, "awesome": 0.9, "amazing": 1.0, "excellent": 1.0,
            "fantastic": 1.0, "wonderful": 0.9, "great": 0.8, "good": 0.7,
            "nice": 0.6, "fine": 0.5, "ok": 0.3, "okay": 0.3,
            "há»¯u Ã­ch": 0.8, "tiá»‡n lá»£i": 0.7, "thuáº­n tiá»‡n": 0.6, "dá»… dÃ ng": 0.6
        }
        
        negative = {
            # CÆ¡ báº£n  
            "tá»‡": -0.8, "kÃ©m": -0.7, "dá»Ÿ": -0.6, "chÃ¡n": -0.5, "buá»“n": -0.6,
            "khÃ³ chá»‹u": -0.7, "bá»±c": -0.8, "giáº­n": -0.9, "ghÃ©t": -0.9, "sá»£": -0.7,
            "lo": -0.5, "lo láº¯ng": -0.6, "tháº¥t vá»ng": -0.8, "tá»“i": -0.7,
            "cháº­m": -0.6, "sai": -0.7, "lá»—i": -0.8, "há»ng": -0.9,
            "xáº¥u": -0.7, "kinh khá»§ng": -0.9, "tá»“i tá»‡": -1.0, "khá»§ng khiáº¿p": -1.0,
            # Má»Ÿ rá»™ng
            "tháº¥t báº¡i": -0.9, "thua": -0.7, "khÃ´ng tá»‘t": -0.6, "khÃ´ng hay": -0.5,
            "boring": -0.5, "bad": -0.7, "terrible": -0.9, "awful": -0.9,
            "horrible": -1.0, "disgusting": -1.0, "hate": -0.9, "angry": -0.8,
            "sad": -0.6, "disappointed": -0.8, "frustrated": -0.7, "annoyed": -0.6,
            "phá»©c táº¡p": -0.4, "khÃ³ khÄƒn": -0.6, "ráº¯c rá»‘i": -0.5, "phiá»n phá»©c": -0.5
        }
        
        # Káº¿t há»£p tá»« Ä‘iá»ƒn
        sentiment_dict = {}
        sentiment_dict.update(positive)
        sentiment_dict.update(negative)
        return sentiment_dict
    
    def _load_synonyms(self) -> Dict[str, List[str]]:
        """Táº£i tá»« Ä‘iá»ƒn tá»« Ä‘á»“ng nghÄ©a"""
        return {
            "xÃ³a": ["há»§y", "loáº¡i bá»", "gá»¡ bá»", "remove", "delete", "xoÃ¡"],
            "táº¡o": ["lÃ m", "táº¡o ra", "thiáº¿t láº­p", "create", "make", "build"],
            "xem": ["hiá»ƒn thá»‹", "liá»‡t kÃª", "kiá»ƒm tra", "show", "view", "list", "display"],
            "nháº¯c nhá»Ÿ": ["reminder", "ghi chÃº", "note", "lá»‹ch", "calendar", "sá»± kiá»‡n", "event"],
            "má»Ÿ": ["khá»Ÿi Ä‘á»™ng", "cháº¡y", "start", "open", "launch", "run"],
            "giá»": ["thá»i gian", "time", "hour"],
            "ngÃ y mai": ["tomorrow", "mai"],
            "hÃ´m nay": ["today", "nay"],
            "tuáº§n nÃ y": ["this week"],
            "thÃ¡ng nÃ y": ["this month"],
            "quan trá»ng": ["important", "Æ°u tiÃªn", "priority", "urgent", "kháº©n cáº¥p"],
            "cuá»™c há»p": ["meeting", "há»p", "há»™i nghá»‹", "conference"],
            "cÃ´ng viá»‡c": ["work", "task", "job", "nhiá»‡m vá»¥", "viá»‡c"],
            "tÃ­nh toÃ¡n": ["calculate", "tÃ­nh", "compute", "math"]
        }

    def analyze_text(self, text: str) -> Dict[str, Any]:
        """PhÃ¢n tÃ­ch vÄƒn báº£n Ä‘áº§u vÃ o vÃ  tráº£ vá» káº¿t quáº£ phÃ¢n tÃ­ch"""
        text = text.lower().strip()
        
        # Káº¿t quáº£ phÃ¢n tÃ­ch
        analysis = {
            "intent": self.detect_intent(text),
            "entities": self.extract_entities(text),
            "sentiment": self.analyze_sentiment(text),
            "keywords": self.extract_keywords(text),
            "normalized_text": self.normalize_text(text),
            "reminder_action": self.analyze_reminder_action(text)
        }
        
        return analysis
        
    def detect_enhanced_intent(self, text: str) -> Dict[str, float]:
        """PhÃ¡t hiá»‡n Ã½ Ä‘á»‹nh Ä‘Æ°á»£c cáº£i tiáº¿n vá»›i scoring system"""
        intent_scores = {}
        
        # Kiá»ƒm tra cÃ¡c pattern vá»›i trá»ng sá»‘ khÃ¡c nhau
        for intent, patterns in self.intent_patterns.items():
            max_score = 0.0
            pattern_matches = 0
            
            for pattern in patterns:
                if re.search(pattern, text, re.IGNORECASE):
                    pattern_matches += 1
                    # Scoring system cáº£i tiáº¿n
                    base_score = 0.4
                    match_bonus = pattern_matches * 0.15
                    score = min(base_score + match_bonus, 1.0)
                    max_score = max(max_score, score)
            
            if max_score > 0:
                intent_scores[intent] = max_score
        
        # Xá»­ lÃ½ Ä‘áº·c biá»‡t cho cÃ¡c intent phá»©c táº¡p
        if any(word in text for word in ["xÃ³a", "há»§y", "delete", "remove"]) and any(word in text for word in ["nháº¯c", "ghi chÃº", "lá»‹ch", "reminder"]):
            intent_scores["delete_reminder"] = 0.95
        
        if any(word in text for word in ["xem", "hiá»ƒn thá»‹", "list", "show"]) and any(word in text for word in ["nháº¯c", "ghi chÃº", "lá»‹ch", "reminder"]):
            intent_scores["list_reminder"] = 0.95
            
        # Fallback mechanism
        if not intent_scores:
            # PhÃ¢n tÃ­ch dá»±a trÃªn cáº¥u trÃºc cÃ¢u
            if "?" in text or text.endswith("khÃ´ng"):
                intent_scores["question"] = 0.6
            elif any(word in text for word in ["lÃ m", "táº¡o", "giÃºp", "má»Ÿ"]):
                intent_scores["command"] = 0.6
            else:
                intent_scores["unknown"] = 0.8
            
        return intent_scores
    
    def extract_enhanced_entities(self, text: str) -> Dict[str, List[str]]:
        """TrÃ­ch xuáº¥t cÃ¡c thá»±c thá»ƒ vá»›i post-processing"""
        entities = {}
        
        for entity_type, patterns in self.entity_patterns.items():
            matches = []
            for pattern in patterns:
                for match in re.finditer(pattern, text, re.IGNORECASE):
                    entity_text = match.group(0).strip()
                    # Post-processing: filter out short or invalid entities
                    if len(entity_text) > 1 and entity_text not in ["cá»§a", "trong", "vá»›i", "vÃ ", "lÃ "]:
                        matches.append(entity_text)
            
            if matches:
                # Remove duplicates while preserving order
                entities[entity_type] = list(dict.fromkeys(matches))
        
        return entities
    
    def analyze_enhanced_sentiment(self, text: str) -> Dict[str, float]:
        """PhÃ¢n tÃ­ch cáº£m xÃºc cáº£i tiáº¿n vá»›i context awareness"""
        words = re.findall(r'\b\w+\b', text.lower())
        phrases = []
        
        # Táº¡o cÃ¡c cá»¥m tá»« 2-3 tá»« liÃªn tiáº¿p
        for i in range(len(words) - 1):
            phrases.append(words[i] + " " + words[i+1])
            if i < len(words) - 2:
                phrases.append(words[i] + " " + words[i+1] + " " + words[i+2])
        
        # TÃ­nh Ä‘iá»ƒm cáº£m xÃºc vá»›i weight khÃ¡c nhau
        score = 0.0
        count = 0
        confidence = 0.0
        
        # Kiá»ƒm tra cá»¥m tá»« trÆ°á»›c (weight cao hÆ¡n)
        for phrase in phrases:
            if phrase in self.sentiment_words:
                weight = 1.5  # Cá»¥m tá»« cÃ³ trá»ng sá»‘ cao hÆ¡n
                score += self.sentiment_words[phrase] * weight
                confidence += abs(self.sentiment_words[phrase]) * weight
                count += weight
        
        # Kiá»ƒm tra tá»«ng tá»«
        for word in words:
            if word in self.sentiment_words:
                score += self.sentiment_words[word]
                confidence += abs(self.sentiment_words[word])
                count += 1
        
        # Xá»­ lÃ½ negation (phá»§ Ä‘á»‹nh)
        negation_words = ["khÃ´ng", "cháº³ng", "cháº£", "Ä‘Ã¢u", "not", "no", "never"]
        has_negation = any(neg in words for neg in negation_words)
        if has_negation and score != 0:
            score *= -0.8  # Äáº£o ngÆ°á»£c vÃ  giáº£m intensity
        
        # Xá»­ lÃ½ intensifiers (tá»« nháº¥n máº¡nh)
        intensifiers = {"ráº¥t": 1.3, "cá»±c": 1.5, "vÃ´ cÃ¹ng": 1.4, "very": 1.3, "really": 1.2, "extremely": 1.5}
        for intensifier, multiplier in intensifiers.items():
            if intensifier in words and score != 0:
                score *= multiplier
                confidence *= multiplier
                break
        
        # TÃ­nh toÃ¡n káº¿t quáº£
        if count > 0:
            avg_score = score / count
            confidence = min(confidence / count, 1.0)
        else:
            avg_score = 0.0
            confidence = 0.0
            
        sentiment = {
            "score": avg_score,
            "confidence": confidence,
            "label": "positive" if avg_score > 0.15 else "negative" if avg_score < -0.15 else "neutral"
        }
        
        return sentiment
    
    def extract_smart_keywords(self, text: str) -> List[str]:
        """TrÃ­ch xuáº¥t tá»« khÃ³a thÃ´ng minh vá»›i frequency analysis"""
        # Chuáº©n hÃ³a text
        text = re.sub(r'[^\w\s\u00C0-\u024F\u1E00-\u1EFF]', ' ', text.lower())
        words = text.split()
        
        # Extended stopwords list
        stopwords = {
            "vÃ ", "hoáº·c", "lÃ ", "cá»§a", "tá»«", "vá»›i", "cÃ¡c", "nhá»¯ng", "má»™t", "cÃ³", "khÃ´ng", 
            "Ä‘Æ°á»£c", "trong", "Ä‘áº¿n", "cho", "vá»", "Ä‘á»ƒ", "theo", "táº¡i", "bá»Ÿi", "vÃ¬", "náº¿u", 
            "khi", "mÃ ", "nhÆ°", "thÃ¬", "nhÆ°ng", "tÃ´i", "báº¡n", "anh", "chá»‹", "há»", "chÃºng", 
            "mÃ¬nh", "nÃ y", "Ä‘Ã³", "Ä‘Ã¢y", "kia", "tháº¿", "váº­y", "rá»“i", "sáº½", "Ä‘Ã£", "Ä‘ang",
            "the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for", "of", 
            "with", "by", "from", "up", "about", "into", "through", "during", "before", 
            "after", "above", "below", "between", "among", "is", "are", "was", "were", 
            "be", "been", "being", "have", "has", "had", "do", "does", "did", "will", 
            "would", "could", "should", "may", "might", "must", "can", "shall"
        }
        
        # Filter words vÃ  tÃ­nh frequency
        filtered_words = [word for word in words if word not in stopwords and len(word) > 2]
        word_freq = Counter(filtered_words)
        
        # Láº¥y top keywords based on frequency vÃ  length
        keywords = []
        for word, freq in word_freq.most_common():
            # Bonus cho tá»« dÃ i hÆ¡n vÃ  frequency cao
            score = freq * (len(word) / 5.0)
            if score >= 1.0:  # Threshold
                keywords.append(word)
            
            if len(keywords) >= 10:  # Limit sá»‘ keywords
                break
        
        return keywords
    
    def _get_main_intent(self, analysis: Dict[str, Any]) -> str:
        """XÃ¡c Ä‘á»‹nh Ã½ Ä‘á»‹nh chÃ­nh tá»« analysis results"""
        intents = analysis.get("intent", {})
        if not intents:
            return "unknown"
        
        # Sáº¯p xáº¿p theo score vÃ  láº¥y intent cao nháº¥t
        sorted_intents = sorted(intents.items(), key=lambda x: x[1], reverse=True)
        return sorted_intents[0][0]
    
    def _is_reminder_related(self, command: str, analysis: Dict[str, Any]) -> bool:
        """Kiá»ƒm tra xem lá»‡nh cÃ³ liÃªn quan Ä‘áº¿n reminder hay khÃ´ng"""
        # Kiá»ƒm tra tá»« khÃ³a trá»±c tiáº¿p
        reminder_keywords = ["nháº¯c", "nháº¯c nhá»Ÿ", "reminder", "ghi chÃº", "lá»‹ch", "schedule", "calendar"]
        if any(keyword in command.lower() for keyword in reminder_keywords):
            return True
        
        # Kiá»ƒm tra intent
        main_intent = self._get_main_intent(analysis)
        if main_intent in ["reminder", "delete_reminder", "list_reminder"]:
            return True
        
        # Kiá»ƒm tra reminder_action
        reminder_action = analysis.get("reminder_action", {})
        if reminder_action.get("action_type"):
            return True
        
        return False
    
    def _update_context(self, command: str, analysis: Dict[str, Any]):
        """Cáº­p nháº­t context memory"""
        context_entry = {
            "command": command,
            "intent": self._get_main_intent(analysis),
            "entities": analysis.get("entities", {}),
            "timestamp": datetime.datetime.now().isoformat()
        }
        
        self.context_memory.append(context_entry)
        
        # Giá»¯ chá»‰ 10 context entries gáº§n nháº¥t
        if len(self.context_memory) > 10:
            self.context_memory.pop(0)
    
    def _calculate_context_score(self, text: str, analysis: Dict[str, Any]) -> float:
        """TÃ­nh Ä‘iá»ƒm relevance dá»±a trÃªn context"""
        if not self.context_memory:
            return 0.0
        
        # Kiá»ƒm tra similarity vá»›i cÃ¡c lá»‡nh trÆ°á»›c Ä‘Ã³
        current_keywords = set(analysis.get("keywords", []))
        total_score = 0.0
        
        for context in self.context_memory[-3:]:  # Chá»‰ xÃ©t 3 context gáº§n nháº¥t
            context_keywords = set()
            for entity_list in context.get("entities", {}).values():
                context_keywords.update(entity_list)
            
            # TÃ­nh Jaccard similarity
            if current_keywords or context_keywords:
                intersection = len(current_keywords.intersection(context_keywords))
                union = len(current_keywords.union(context_keywords))
                similarity = intersection / union if union > 0 else 0
                total_score += similarity
        
        return min(total_score / 3, 1.0) if self.context_memory else 0.0
    
    def _assess_complexity(self, text: str, analysis: Dict[str, Any]) -> str:
        """ÄÃ¡nh giÃ¡ Ä‘á»™ phá»©c táº¡p cá»§a command"""
        word_count = len(text.split())
        entity_count = sum(len(entities) for entities in analysis.get("entities", {}).values())
        intent_count = len(analysis.get("intent", {}))
        
        complexity_score = word_count * 0.3 + entity_count * 0.5 + intent_count * 0.2
        
        if complexity_score < 3:
            return "simple"
        elif complexity_score < 7:
            return "medium"
        else:
            return "complex"
    
    def _calculate_confidence(self, analysis: Dict[str, Any]) -> float:
        """TÃ­nh confidence score cho analysis"""
        intents = analysis.get("intent", {})
        entities = analysis.get("entities", {})
        sentiment = analysis.get("sentiment", {})
        
        # Intent confidence
        intent_conf = max(intents.values()) if intents else 0.0
        
        # Entity confidence (based on count)
        entity_conf = min(len(entities) * 0.2, 1.0)
        
        # Sentiment confidence
        sentiment_conf = sentiment.get("confidence", 0.0) if sentiment else 0.0
        
        # Weighted average
        overall_confidence = (intent_conf * 0.5 + entity_conf * 0.3 + sentiment_conf * 0.2)
        
        return min(overall_confidence, 1.0)
    
    def _format_analysis_result(self, analysis: Dict[str, Any]) -> str:
        """Format detailed analysis results"""
        result = "ğŸ§  Káº¿t quáº£ phÃ¢n tÃ­ch ngÃ´n ngá»¯ cáº£i tiáº¿n:\n\n"
        
        # Intent vá»›i confidence
        intents = analysis.get("intent", {})
        if intents:
            result += "ğŸ¯ Ã Ä‘á»‹nh: "
            sorted_intents = sorted(intents.items(), key=lambda x: x[1], reverse=True)
            intent_name = sorted_intents[0][0]
            intent_score = sorted_intents[0][1]
            
            intent_map = {
                "question": "CÃ¢u há»i",
                "command": "YÃªu cáº§u/Má»‡nh lá»‡nh",
                "reminder": "Táº¡o nháº¯c nhá»Ÿ",
                "delete_reminder": "XÃ³a nháº¯c nhá»Ÿ",
                "list_reminder": "Xem nháº¯c nhá»Ÿ",
                "greeting": "Lá»i chÃ o",
                "farewell": "Lá»i táº¡m biá»‡t",
                "thanks": "Lá»i cáº£m Æ¡n",
                "apology": "Lá»i xin lá»—i",
                "unknown": "KhÃ´ng xÃ¡c Ä‘á»‹nh"
            }
            result += f"{intent_map.get(intent_name, intent_name)} ({intent_score:.2f})\n\n"
        
        # Sentiment vá»›i confidence
        sentiment = analysis.get("sentiment", {})
        if sentiment:
            result += "ğŸ˜Š Cáº£m xÃºc: "
            sentiment_map = {
                "positive": "TÃ­ch cá»±c",
                "negative": "TiÃªu cá»±c", 
                "neutral": "Trung tÃ­nh"
            }
            label = sentiment_map.get(sentiment.get('label'), sentiment.get('label', 'Trung tÃ­nh'))
            score = sentiment.get('score', 0)
            confidence = sentiment.get('confidence', 0)
            result += f"{label} (Ä‘iá»ƒm: {score:.2f}, tin cáº­y: {confidence:.2f})\n\n"
        
        # Entities
        entities = analysis.get("entities", {})
        if entities:
            result += "ğŸ·ï¸ Thá»±c thá»ƒ:\n"
            entity_map = {
                "date": "NgÃ y thÃ¡ng",
                "time": "Thá»i gian",
                "person": "NgÆ°á»i",
                "location": "Äá»‹a Ä‘iá»ƒm",
                "number": "Sá»‘",
                "priority": "Æ¯u tiÃªn",
                "duration": "Thá»i lÆ°á»£ng"
            }
            for entity_type, entity_list in entities.items():
                result += f"  - {entity_map.get(entity_type, entity_type)}: {', '.join(entity_list)}\n"
            result += "\n"
        
        # Keywords
        keywords = analysis.get("keywords", [])
        if keywords:
            result += f"ğŸ”‘ Tá»« khÃ³a: {', '.join(keywords[:8])}\n\n"  # Limit to 8 keywords
        
        # Advanced metrics
        context_score = analysis.get("context_score", 0)
        complexity = analysis.get("complexity", "unknown")
        confidence = analysis.get("confidence", 0)
        
        result += f"ğŸ“Š PhÃ¢n tÃ­ch nÃ¢ng cao:\n"
        result += f"  - Äiá»ƒm ngá»¯ cáº£nh: {context_score:.2f}\n"
        result += f"  - Äá»™ phá»©c táº¡p: {complexity}\n"
        result += f"  - Äá»™ tin cáº­y tá»•ng thá»ƒ: {confidence:.2f}\n\n"
        
        # Normalized text
        result += f"ğŸ“ VÄƒn báº£n chuáº©n hÃ³a: {analysis.get('normalized_text', '')}"
        
        return result
        
    def analyze_reminder_action(self, text: str) -> Dict[str, Any]:
        """PhÃ¢n tÃ­ch hÃ nh Ä‘á»™ng liÃªn quan Ä‘áº¿n nháº¯c nhá»Ÿ vÃ  ghi chÃº"""
        result = {
            "action_type": None,
            "reminder_id": None,
            "reminder_content": None,
            "time_info": None
        }
        
        # XÃ¡c Ä‘á»‹nh loáº¡i hÃ nh Ä‘á»™ng
        if any(word in text for word in ["xÃ³a", "há»§y", "loáº¡i bá»", "bá»", "xoÃ¡", "huá»·", "gá»¡ bá»"]):
            result["action_type"] = "delete"
        elif any(word in text for word in ["thÃªm", "táº¡o", "Ä‘áº·t", "lÃªn lá»‹ch", "nháº¯c"]):
            result["action_type"] = "add"
        elif any(word in text for word in ["xem", "hiá»ƒn thá»‹", "liá»‡t kÃª", "kiá»ƒm tra", "cÃ³ gÃ¬", "cÃ³ nhá»¯ng"]):
            result["action_type"] = "list"
        elif any(word in text for word in ["cáº­p nháº­t", "sá»­a", "chá»‰nh sá»­a", "thay Ä‘á»•i"]):
            result["action_type"] = "update"
        
        # TÃ¬m ID nháº¯c nhá»Ÿ
        id_match = re.search(r'id[:\s]*(\d+)', text, re.IGNORECASE)
        if id_match:
            result["reminder_id"] = id_match.group(1)
        
        # TÃ¬m ná»™i dung nháº¯c nhá»Ÿ (sau cÃ¡c tá»« khÃ³a hÃ nh Ä‘á»™ng)
        if result["action_type"] in ["delete", "update"]:
            action_keywords = {
                "delete": ["xÃ³a", "há»§y", "loáº¡i bá»", "bá»", "xoÃ¡", "huá»·", "gá»¡ bá»"],
                "update": ["cáº­p nháº­t", "sá»­a", "chá»‰nh sá»­a", "thay Ä‘á»•i"]
            }
            
            for keyword in action_keywords[result["action_type"]]:
                if keyword in text:
                    parts = text.split(keyword, 1)
                    if len(parts) > 1:
                        content = parts[1].strip()
                        # Loáº¡i bá» cÃ¡c tá»« khÃ´ng cáº§n thiáº¿t
                        for word in ["nháº¯c nhá»Ÿ", "ghi chÃº", "lá»‹ch", "sá»± kiá»‡n", "giÃºp", "tÃ´i", "cho", "cá»§a"]:
                            content = content.replace(word, "").strip()
                        
                        if content:
                            result["reminder_content"] = content
                            break
        
        # TÃ¬m thÃ´ng tin thá»i gian
        time_entities = self.extract_entities(text).get("date", []) + self.extract_entities(text).get("time", [])
        if time_entities:
            result["time_info"] = time_entities
        
        return result
    
    def detect_intent(self, text: str) -> Dict[str, float]:
        """PhÃ¡t hiá»‡n Ã½ Ä‘á»‹nh tá»« vÄƒn báº£n"""
        intent_scores = {}
        
        # Kiá»ƒm tra cÃ¡c pattern vá»›i trá»ng sá»‘ khÃ¡c nhau
        for intent, patterns in self.intent_patterns.items():
            max_score = 0.0
            pattern_matches = 0
            
            for pattern in patterns:
                if re.search(pattern, text):
                    pattern_matches += 1
                    score = 0.5 + (pattern_matches * 0.2)  # TÄƒng Ä‘iá»ƒm theo sá»‘ pattern khá»›p
                    max_score = max(max_score, min(score, 1.0))
            
            if max_score > 0:
                intent_scores[intent] = max_score
        
        # Xá»­ lÃ½ Ä‘áº·c biá»‡t cho cÃ¡c intent phá»©c táº¡p
        if any(word in text for word in ["xÃ³a", "há»§y", "delete", "remove"]) and any(word in text for word in ["nháº¯c", "ghi chÃº", "lá»‹ch", "reminder"]):
            intent_scores["delete_reminder"] = 0.9
        
        if any(word in text for word in ["xem", "hiá»ƒn thá»‹", "list", "show"]) and any(word in text for word in ["nháº¯c", "ghi chÃº", "lá»‹ch", "reminder"]):
            intent_scores["list_reminder"] = 0.9
            
        # Náº¿u khÃ´ng phÃ¡t hiá»‡n Ä‘Æ°á»£c Ã½ Ä‘á»‹nh rÃµ rÃ ng
        if not intent_scores:
            intent_scores["unknown"] = 1.0
            
        return intent_scores
    
    def extract_entities(self, text: str) -> Dict[str, List[str]]:
        """TrÃ­ch xuáº¥t cÃ¡c thá»±c thá»ƒ tá»« vÄƒn báº£n"""
        entities = {}
        
        for entity_type, patterns in self.entity_patterns.items():
            matches = []
            for pattern in patterns:
                for match in re.finditer(pattern, text):
                    matches.append(match.group(0))
            
            if matches:
                entities[entity_type] = matches
        
        return entities
    
    def analyze_sentiment(self, text: str) -> Dict[str, float]:
        """PhÃ¢n tÃ­ch cáº£m xÃºc trong vÄƒn báº£n"""
        words = re.findall(r'\b\w+\b', text)
        phrases = []
        
        # Táº¡o cÃ¡c cá»¥m tá»« 2 tá»« liÃªn tiáº¿p
        for i in range(len(words) - 1):
            phrases.append(words[i] + " " + words[i+1])
        
        # TÃ­nh Ä‘iá»ƒm cáº£m xÃºc
        score = 0.0
        count = 0
        
        # Kiá»ƒm tra cá»¥m tá»« trÆ°á»›c
        for phrase in phrases:
            if phrase in self.sentiment_words:
                score += self.sentiment_words[phrase]
                count += 1
        
        # Kiá»ƒm tra tá»«ng tá»«
        for word in words:
            if word in self.sentiment_words:
                score += self.sentiment_words[word]
                count += 1
        
        # XÃ¡c Ä‘á»‹nh cáº£m xÃºc
        if count > 0:
            avg_score = score / count
        else:
            avg_score = 0.0
            
        sentiment = {
            "score": avg_score,
            "label": "positive" if avg_score > 0.1 else "negative" if avg_score < -0.1 else "neutral"
        }
        
        return sentiment
    
    def extract_keywords(self, text: str) -> List[str]:
        """TrÃ­ch xuáº¥t tá»« khÃ³a quan trá»ng tá»« vÄƒn báº£n"""
        # Loáº¡i bá» dáº¥u cÃ¢u
        text = text.translate(str.maketrans("", "", string.punctuation))
        
        # TÃ¡ch tá»«
        words = text.split()
        
        # Loáº¡i bá» cÃ¡c tá»« dá»«ng (stopwords)
        stopwords = ["vÃ ", "hoáº·c", "lÃ ", "cá»§a", "tá»«", "vá»›i", "cÃ¡c", "nhá»¯ng", "má»™t", "cÃ³", "khÃ´ng", 
                    "Ä‘Æ°á»£c", "trong", "Ä‘áº¿n", "cho", "vá»", "Ä‘á»ƒ", "theo", "táº¡i", "bá»Ÿi", "vÃ¬", "náº¿u", 
                    "khi", "mÃ ", "nhÆ°", "thÃ¬", "nhÆ°ng", "tÃ´i", "báº¡n", "anh", "chá»‹", "há»", "chÃºng", 
                    "mÃ¬nh", "nÃ y", "Ä‘Ã³", "Ä‘Ã¢y", "kia", "tháº¿", "váº­y"]
        
        keywords = [word for word in words if word not in stopwords and len(word) > 1]
        
        # Tráº£ vá» danh sÃ¡ch tá»« khÃ³a duy nháº¥t
        return list(set(keywords))
    
    def normalize_text(self, text: str) -> str:
        """Chuáº©n hÃ³a vÄƒn báº£n"""
        # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a
        text = re.sub(r'\s+', ' ', text).strip()
        
        # Chuáº©n hÃ³a dáº¥u cÃ¢u
        text = re.sub(r'\s*([,.!?;:])\s*', r'\1 ', text)
        
        # Chuáº©n hÃ³a chá»¯ hoa Ä‘áº§u cÃ¢u
        sentences = re.split(r'([.!?]\s)', text)
        normalized_text = ""
        
        for i in range(0, len(sentences), 2):
            if i < len(sentences):
                sentence = sentences[i].strip()
                if sentence:
                    sentence = sentence[0].upper() + sentence[1:]
                    normalized_text += sentence
                
                if i + 1 < len(sentences):
                    normalized_text += sentences[i + 1]
        
        return normalized_text

# Singleton instance
_nlp_processor = None

def get_nlp_processor() -> EnhancedNLPProcessor:
    """Tráº£ vá» instance cá»§a EnhancedNLPProcessor (singleton)"""
    global _nlp_processor
    if _nlp_processor is None:
        _nlp_processor = EnhancedNLPProcessor()
    return _nlp_processor

def analyze_user_input(text: str) -> Dict[str, Any]:
    """PhÃ¢n tÃ­ch Ä‘áº§u vÃ o cá»§a ngÆ°á»i dÃ¹ng vá»›i enhanced capabilities"""
    processor = get_nlp_processor()
    return processor.analyze_text_with_context(text)

def enhance_with_nlp(command: str) -> str:
    """HÃ m chÃ­nh Ä‘á»ƒ xá»­ lÃ½ lá»‡nh vá»›i NLP"""
    if not command:
        return "Vui lÃ²ng cung cáº¥p vÄƒn báº£n Ä‘á»ƒ phÃ¢n tÃ­ch."
    
    # PhÃ¢n tÃ­ch vÄƒn báº£n
    analysis = analyze_user_input(command)
    
    # Táº¡o káº¿t quáº£ phÃ¢n tÃ­ch
    result = "ğŸ“Š Káº¿t quáº£ phÃ¢n tÃ­ch ngÃ´n ngá»¯:\n\n"
    
    # Ã Ä‘á»‹nh
    result += "ğŸ¯ Ã Ä‘á»‹nh: "
    intents = sorted(analysis["intent"].items(), key=lambda x: x[1], reverse=True)
    if intents:
        intent_name = intents[0][0]
        intent_map = {
            "question": "CÃ¢u há»i",
            "command": "YÃªu cáº§u/Má»‡nh lá»‡nh",
            "greeting": "Lá»i chÃ o",
            "farewell": "Lá»i táº¡m biá»‡t",
            "thanks": "Lá»i cáº£m Æ¡n",
            "apology": "Lá»i xin lá»—i",
            "unknown": "KhÃ´ng xÃ¡c Ä‘á»‹nh"
        }
        result += f"{intent_map.get(intent_name, intent_name)} ({intents[0][1]:.2f})\n"
    else:
        result += "KhÃ´ng xÃ¡c Ä‘á»‹nh\n"
    
    # Cáº£m xÃºc
    sentiment = analysis["sentiment"]
    result += "ğŸ˜Š Cáº£m xÃºc: "
    sentiment_map = {
        "positive": "TÃ­ch cá»±c",
        "negative": "TiÃªu cá»±c",
        "neutral": "Trung tÃ­nh"
    }
    result += f"{sentiment_map.get(sentiment['label'], sentiment['label'])} ({sentiment['score']:.2f})\n"
    
    # Thá»±c thá»ƒ
    if analysis["entities"]:
        result += "ğŸ·ï¸ Thá»±c thá»ƒ:\n"
        entity_map = {
            "date": "NgÃ y thÃ¡ng",
            "time": "Thá»i gian",
            "person": "NgÆ°á»i",
            "location": "Äá»‹a Ä‘iá»ƒm",
            "number": "Sá»‘"
        }
        for entity_type, entities in analysis["entities"].items():
            result += f"  - {entity_map.get(entity_type, entity_type)}: {', '.join(entities)}\n"
    
    # Tá»« khÃ³a
    if analysis["keywords"]:
        result += "ğŸ”‘ Tá»« khÃ³a: " + ", ".join(analysis["keywords"]) + "\n"
    
    # VÄƒn báº£n chuáº©n hÃ³a
    result += "ğŸ“ VÄƒn báº£n chuáº©n hÃ³a: " + analysis["normalized_text"]
    
    return result